{
  
    
        "post0": {
            "title": "Segmentation Model-Part IV - Data augmentation on the GPU with Kornia library",
            "content": "In this post, we discover how to use Kornia modules in order to perform the data augmentatio on the GPU in batch mode. Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Kornia consists a lot of components. One of them is kornia.augmentation - a module to perform data augmentation in the GPU. . We will work with the Segmentation Problem (Nail Segmentation). For that, we use Pytorch Lightninig to train model and use Kornia to build the data augmentation on the GPU. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . Similar to the training pipeline of the previous post, we first make the data frame to store images and masks infos. . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. . 3. The CPU bottleneck . The fact is that today these transforms are applied one input at a time on CPUs. This means that they are super slow. . 3.1 A naive approach model training . . A naive training pipeline includes: . The pre-processing of the data occurs on the CPU | The model will be typically trained on GPU/TPU. | . 3.2 Data Augmentation using GPU . To improve the training speed we can shift the data augmentation task in to GPU . . To do that we can use Kornia.augmentation, Dali. . Kornia.augmentation is the module of Kornia which permit to do augmentation in GPU. It will boost the speed of traininig in almost cases. | DALI is a library for data loading and pre-processing to accelerate deep learning applications. Data processing pipelines implemented using DALI can easily be retargeted to TensorFlow, PyTorch[https://pytorch.org/], MXNet and PaddlePaddle. This post we will focus on how to use Kornia. The guide of using DALI will be introduced in next post. | . 4. Data Augmentation using Kornia . In this part, we will cover how to use Kornia for data augmentation. To augumentate data on GPU, we can understand transforms (augumentations) as a transform_module ( is a nn.Module object) whose input is a tensor of size $C times H times W$ and output is also tensor of size $C times H times W$. . That transform_module is put between the processing task (includes read images, make images of batch having same size, convert images in to the tensor format) and the training model. More precisely, . class ModelWithAugumentation(nn.Module): &quot;&quot;&quot;Module to perform data augmentation on torch tensors.&quot;&quot;&quot; def __init__(self, transform_module: nn.Module, model : nn.Module) -&gt; None: super().__init__() self.transform_module = transform_module self.model = model def forward(self, x: Tensor) -&gt; Tensor: augmented_x = self.transform_module(x) # BxCxHxW x_out = self.model(augmented_x) return x_out . where transform_module is defined by using Kornia or torchvision. For example . transform_module = K.augmentation.AugmentationSequential( K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1) ) . We now apply that strategy to our problem. Comparing with the previous pipeline in the last post (Training deep learning segmentation models in Pytorch Lightning), here are some modifications. . Only use Resize or Padding in the data augmentation on CPUs, in the last part we define the whole augmentation by using albumentations and use it as the transform before going to the model. | . self.valid_transform = resize() self.train_transform = resize() . Using Kornia to define the augmentation, hare we have train_transform_K and valid_transform_K | . valid_transform_K = K.augmentation.AugmentationSequential( K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1), data_keys=[&quot;input&quot;, &quot;mask&quot;], ) train_transform_K = K.augmentation.AugmentationSequential( K.augmentation.container.ImageSequential( # OneOf K.augmentation.RandomHorizontalFlip(p=0.6), K.augmentation.RandomVerticalFlip(p=0.6), random_apply=1, random_apply_weights=[0.5, 0.5], ), K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5), # K.augmentation.RandomAffine( degrees = (-15.0,15.0), p= 0.3), K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1), data_keys=[&quot;input&quot;, &quot;mask&quot;], same_on_batch=False, ) . In the LightningModule, we define two new functions self.train_transform = train_transform_K self.valid_transform = valid_transform_K . and add transform into the training loop and the valid loop (training_step and validation_step) . | . def training_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] if self.train_transform is not None: imgs, masks = self.train_transform(imgs, masks) # add the transform before going to the model imgs, masks = imgs.float(), masks.float() logits = self(imgs) train_loss = self.loss_function(logits, masks) train_dice_soft = self.dice_soft(logits, masks) self.log(&quot;train_loss&quot;, train_loss, prog_bar=True) self.log(&quot;train_dice_soft&quot;, train_dice_soft, prog_bar=True) return {&quot;loss&quot;: train_loss, &quot;train_dice_soft&quot;: train_dice_soft} def validation_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] if self.valid_transform: imgs, masks = self.valid_transform(imgs, masks) # add the transform before going to the model imgs, masks = imgs.float(), masks.float() logits = self(imgs) valid_loss = self.loss_function(logits, masks) valid_dice_soft = self.dice_soft(logits, masks) valid_iou = binary_mean_iou(logits, masks) self.log(&quot;valid_loss&quot;, valid_loss, prog_bar=True) self.log(&quot;valid_dice&quot;, valid_dice_soft, prog_bar=True) self.log(&quot;valid_iou&quot;, valid_iou, prog_bar=True) return { &quot;valid_loss&quot;: valid_loss, &quot;valid_dice&quot;: valid_dice_soft, &quot;valid_iou&quot;: valid_iou, } . We keep all of rest parts of the pipeline (LightningDataModule, Trainer). . For more details, we can find the source code at github . References . Segmentation Model-Part III - Training deep learning segmentation models in Pytorch Lightning | Kornia.augmentation | .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2022/08/04/segmentation-model-part4.html",
            "relUrl": "/2022/08/04/segmentation-model-part4.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Segmentation Model-Part III - Training deep learning segmentation models in Pytorch Lightning",
            "content": "Continue Segmentation Model the series; in this post, we discuss how to train a segmentation model in Pytorch Lightning. PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. It is built on top of PyTorch. . We still work with the Segmentation Problem (Nail Segmentation) and discover some valuable tools for Pytorch Lightning. From this part, we will focus on the Pytorch Platform. Then for convenience, we recall some tasks of the previous post: Problem Description and Dataset, Data Preparation. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . We want the CSV file that stores the image and mask paths. In this project, file names of images and masks are the same, and then we only need to save the images path and modify the data_root of images and masks when we define a dataset. . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. More precisely, . def make_csv_file(data_root: Union[str, Path]) -&gt; None: list_images_train_masks = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;masks&quot;)) list_images_train_images = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;images&quot;)) list_images_train = [ i for i in list_images_train_images if i in list_images_train_masks ] print(len(list_images_train)) list_images_valid = get_all_items(os.path.join(data_root, &quot;valid&quot;, &quot;masks&quot;)) train_frame = pd.DataFrame(list_images_train, columns=[&quot;images&quot;]) train_frame[&quot;train&quot;] = 1 valid_frame = pd.DataFrame(list_images_valid, columns=[&quot;images&quot;]) valid_frame[&quot;train&quot;] = 0 mkdir(f&quot;{data_root}/csv_file&quot;) train_frame.to_csv(f&quot;{data_root}/csv_file/train.csv&quot;, index=False) valid_frame.to_csv(f&quot;{data_root}/csv_file/valid.csv&quot;, index=False) . Where get_all_items, mkdir are two supported functions (defined in utils.py file) that help us to find all items in a given folder and make a new folder. . Before going define the dataloader and model, let’s recall some main features of Pytorch Lightning. For more information, you can find it at Pytorch Lightning. . 3. Pytorch Lightnining . PyTorch Lightning is an open-source, lightweight Python wrapper for machine learning researchers that is built on top of PyTorch. . With this framework, you don’t have to remember all the tiny details of the PyTorch framework because Pytorch Lightnining handles it. . Three main features of Pytorch Lightning: . LightningDataModule | LightningModule | Trainer | . 3.1 LightningDataModule . LightningDataModule is a shareable, reusable class that encapsulates all the steps needed to process data: . Data processing | Load inside Dataset | Apply transforms | Wrap inside a DataLoader | . . 3.2 LightningModule . A lightning module is composed of some components that fully define the system: . The model or system of models | The optimizer(s) | The train loop | The validation loop | . 3.3 Trainer . Once we declare LightningDataModule, LightningModule, we can train the model with Trainer API. . . A basic use of trainer: . modelmodule = LightningModule(*args_model) datamodule = LightningDataModule(*args_data) trainer = Trainer(*args_trainer) trainer.fit(modelmodule, datamodule) . 4. DataLoader . To define the LightningModule of our dataset, we first define the torch.utils.data.Dataset for the nail data. . 4.1 Define torch.utils.data.Dataset for the Nail Data . class NailDataset(Dataset): def __init__(self, data_root: str, csv_folder: str, train: str, tfms: A.Compose): self.data_root = data_root self.csv_folder = csv_folder self.train = train self.tfms = tfms if self.train == &quot;train&quot;: self.ids = pd.read_csv(os.path.join(self.csv_folder, &quot;train.csv&quot;))[&quot;images&quot;] else: self.ids = pd.read_csv(os.path.join(self.csv_folder, &quot;valid.csv&quot;))[&quot;images&quot;] def __len__(self) -&gt; int: return len(self.ids) def __getitem__(self, idx: int) -&gt; Any: fname = self.ids[idx] image = read_image(self.data_root + f&quot;/{self.train}/images&quot; + fname) mask = read_mask(self.data_root + f&quot;/{self.train}/masks&quot; + fname) mask = (mask &gt; 0).astype(np.uint8) if self.tfms is not None: augmented = self.tfms(image=image, mask=mask) image, mask = augmented[&quot;image&quot;], augmented[&quot;mask&quot;] return { &quot;image&quot;: img2tensor(image), &quot;label&quot;: img2tensor(mask), } . 4.2 Define LightningDataModule for the Nail Data . We then use LightningDataModule to wrap our NailDataset into the data module of Pytorch Lightning. . class NailSegmentation(LightningDataModule): def __init__(self, data_root: str, csv_path: str, test_path: str, batch_size: int = 16, num_workers: int = 4): super().__init__() assert os.path.isdir(csv_path), f&quot;missing folder: {csv_path}&quot; assert os.path.isdir(data_root), f&quot;missing folder: {data_root}&quot; self.data_root = str(data_root) self.csv_path = str(csv_path) self.test_path = str(test_path) self.valid_transform = valid_transform() self.train_transform = train_transform() # other configs self.batch_size = batch_size self.num_workers = num_workers if num_workers is not None else mproc.cpu_count() def prepare_data(self) -&gt; None: pass def setup(self, *_, **__) -&gt; None: self.train_dataset = NailDataset( self.data_root, self.csv_path, train=&quot;train&quot;, tfms=self.train_transform, ) self.valid_dataset = NailDataset( self.data_root, self.csv_path, train=&quot;valid&quot;, tfms=self.valid_transform, ) def train_dataloader(self) -&gt; DataLoader: return DataLoader( self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, ) def val_dataloader(self) -&gt; DataLoader: return DataLoader( self.valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, ) . Here we need to define 3 main functions: . def train_dataloader(self) | val_dataloader(self) | . Those respond to DataLoader of train and valid dataset in Pytorch. . 5. Model Module . In this part we define: . A segmentation model | Wrap the model module by using LightningModule, for that we will define some main functions: def training_step : calculate {loss, metric}, logging in each train step | def validation_step: calculate {loss, metric}, logging in each valid step | def validation_epoch_end: calculate {loss, metric}, logging in each epoch by using infos of validation_step | def configure_optimizers: which optimization and learning rate scheduler do we use for the training? | . | . 5.1 Define the model by using segmentation_models_pytorch . For convenience, we use segmentation_models_pytorch to define our model. Segmentation_models_pytorch is a high-level API, it helps us build a semantic segmentation model with only some lines of code. . import segmentation_models_pytorch as smp model = smp.Unet( encoder_name=&quot;timm-efficientnet-b4&quot;, # choose encoder, e.g. mobilenet_v2 or efficientnet-b7 encoder_weights=&quot;imagenet&quot;, # use `imagenet` pre-trained weights for encoder initialization in_channels=3, # model input channels (1 for gray-scale images, 3 for RGB, classes=1, # model output channels (number of classes in your dataset) ) . 5.2 Define LightningModule . We next use LightningModule to wrap the model into the model module of Pytorch Lightnining. . class LitNailSegmentation(LightningModule): def __init__(self, model: nn.Module, learning_rate: float = 1e-4): super().__init__() self.model = model self.loss_function = symmetric_lovasz self.dice_soft = binary_dice_coefficient self.learning_rate = learning_rate self.save_hyperparameters() def forward(self, x): return self.model(x) def training_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] imgs, masks = imgs.float(), masks.float() logits = self(imgs) train_loss = self.loss_function(logits, masks) train_dice_soft = self.dice_soft(logits, masks) self.log(&quot;train_loss&quot;, train_loss, prog_bar=True) self.log(&quot;train_dice_soft&quot;, train_dice_soft, prog_bar=True) return {&quot;loss&quot;: train_loss, &quot;train_dice_soft&quot;: train_dice_soft} def validation_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] imgs, masks = imgs.float(), masks.float() logits = self(imgs) valid_loss = self.loss_function(logits, masks) valid_dice_soft = self.dice_soft(logits, masks) valid_iou = binary_mean_iou(logits, masks) self.log(&quot;valid_loss&quot;, valid_loss, prog_bar=True) self.log(&quot;valid_dice&quot;, valid_dice_soft, prog_bar=True) self.log(&quot;valid_iou&quot;, valid_iou, prog_bar=True) return { &quot;valid_loss&quot;: valid_loss, &quot;valid_dice&quot;: valid_dice_soft, &quot;valid_iou&quot;: valid_iou, } def validation_epoch_end(self, outputs): logs = {&quot;epoch&quot;: self.trainer.current_epoch} valid_losses = torch.stack([x[&quot;valid_loss&quot;] for x in outputs]).mean() valid_dices = torch.stack([x[&quot;valid_dice&quot;] for x in outputs]).mean() valid_ious = torch.stack([x[&quot;valid_iou&quot;] for x in outputs]).mean() logs[&quot;valid_losses&quot;] = valid_losses logs[&quot;valid_dices&quot;] = valid_dices logs[&quot;valid_ious&quot;] = valid_ious return { &quot;valid_losses&quot;: valid_losses, &quot;valid_dices&quot;: valid_dices, &quot;valid_ious&quot;: valid_ious, &quot;log&quot;: logs, } def configure_optimizers(self): optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.trainer.max_epochs, 0) self.optimizer = [optimizer] return self.optimizer, [scheduler] . Here we use: . AdamW as the optimizers | symmetric_lovasz as the loss function, which is defined in the Loss.py file. symmetric_lovasz is defined by | . def symmetric_lovasz(outputs, targets): return 0.5*(lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets)) . where lovasz_hinge is Lovasz loss for the binary segmentation. . Metrics: Dice, IOU | . 6. Trainer . Once we have the data module, and model module, we can train the model with Trainer API, . datamodule = NailSegmentation( data_root=data_root, csv_path=csv_path, test_path=&quot;&quot;, batch_size=batch_size, num_workers=4, ) model_lighning = LitNailSegmentation(model=model, learning_rate=config.training.learning_rate) trainer = Trainer(*args_trainer) trainer.fit( model=model_lighning, datamodule=datamodule, ckpt_path=ckpt_path, ) . Here args_trainer is the argument of the trainer. More precisely, it has . { gpus: [0] # gpu device to train max_epochs: 300 # number of epochs precision: 16 # using mix precision to train auto_lr_find: True # auto find the good initial learning rate limit_train_batches: 1.0 # percent of train dataset use to train, here 100% ... } . Lightning implements various techniques to help during training that can help make the training smoother. . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2022/08/03/segmentation-model-part3.html",
            "relUrl": "/2022/08/03/segmentation-model-part3.html",
            "date": " • Aug 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Segmentation Model-Part II - How to handle Imbalanced Data in Segmentation Problem",
            "content": "In the last post, we discussed how to train a segmentation model in Tensorflow. This part will cover how to balance datasets in training a segmentation model in Tensorflow. We can use the same technique to deal with the imbalanced data in a Classification problem. Let us recall our segmentation problem. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the mail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks, each folder has four sub-folders 1, 2, 3, 4 corresponds to four types of distribution of nails. Images is the data folder and Masks is the label folder, which is the segmentations of input images. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . For the same task as the previous post, we want to have the CSV file that stores the image and mask paths . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. What thing do we need more for data balancing? . We remark that our image data have four subfolders, and the distributions of the coverage segmentation are very different in each folder. Also, the quality of the image those are different (skew data). . Folder number of image . 0 | 749 | . 1 | 144 | . 2 | 126 | . 3 | 52 | . 4 | 34 | . We want to split the info data frame into some smaller data frame. To do that we use: . def split_data_train(data_root) -&gt; None: r&quot;&quot;&quot; This function is to split the train into some subsets. The purpose of this step is to make the balanced dataset. &quot;&quot;&quot; data_root = args.data_root path_csv = f&quot;{data_root}/csv_file/train.csv&quot; train = pd.read_csv(path_csv) train[&quot;type&quot;] = train[&quot;images&quot;].apply(lambda x: x.split(&quot;/&quot;)[1]) for i in train[&quot;type&quot;].unique().tolist(): df = train.loc[train[&quot;type&quot;] == i] df.to_csv(f&quot;{data_root}/csv_file/train{i}.csv&quot;, index=False) . We have five new data frame train_0, train_1, train_2, train_3, train_4. We will use those files in the next step. . We will inherit all of the things in the previous post (DataLoader, Model, mixed precision, logger). We need to change how we load datasets and how to balance the data when we load data. . 3. How to define dataloader . We remark that all functions we will use have been defined in the last part. . For more details, we can find the source code at github . We first define all data frame and load directories of image and masks . train0_csv_dir = f&quot;{data_root}/csv_file/train0.csv&quot; train1_csv_dir = f&quot;{data_root}/csv_file/train1.csv&quot; train2_csv_dir = f&quot;{data_root}/csv_file/train2.csv&quot; train3_csv_dir = f&quot;{data_root}/csv_file/train3.csv&quot; train4_csv_dir = f&quot;{data_root}/csv_file/train4.csv&quot; train0_dataset = load_data_path(data_root, train0_csv_dir, &quot;train&quot;) train1_dataset = load_data_path(data_root, train1_csv_dir, &quot;train&quot;) train2_dataset = load_data_path(data_root, train2_csv_dir, &quot;train&quot;) train3_dataset = load_data_path(data_root, train3_csv_dir, &quot;train&quot;) train4_dataset = load_data_path(data_root, train4_csv_dir, &quot;train&quot;) . Using tf_dataset we load five datasets and remark that we will not batch in this step, we will concatenate those datasets with weights and batch them when we have the whole dataset. . The cool thing about this method is that we can use different augmentation for different sub-dataset. For example we can apply the train_transform for the first dataset and valid_transform for the second datset. . train0_loader = tf_dataset( dataset=train0_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train1_loader = tf_dataset( dataset=train1_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train2_loader = tf_dataset( dataset=train2_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train3_loader = tf_dataset( dataset=train3_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train4_loader = tf_dataset( dataset=train4_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) . Shuffle and repeat each dataset . data_loaders = [ train0_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train1_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train2_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train3_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train4_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), ] . Calculate the weighted sample; here we want each batch; each dataset will be loaded with the same sample. . weights = [1 / len(data_loaders)] * len(data_loaders) . Using tf.data.experimental.sample_from_datasets to balance data. . The input tf.data.experimental.sample_from_datasets function is: . datasets: A non-empty list of tf.data.Dataset objects with compatible structure. | weights: (Optional.) A list or Tensor of len(datasets) floating-point values where weights[i] represents the probability to sample from datasets[i], or a tf.data.Dataset object where each element is such a list. Defaults to a uniform distribution across datasets. | . Returns of tf.data.experimental.sample_from_datasets . A dataset that interleaves elements from datasets at random, according to weights if provided, otherwise with uniform probability. | . train_loader = tf.data.experimental.sample_from_datasets(data_loaders, weights=weights, seed=None) . We then have the train_loader with balancing data. We only need to batch them before feeding data into the model. . train_loader = train_loader.batch(batch_size) . Once we have train_loader, we define valid_loader, model, as same as the previous post. Finally, we fit the model. . history = model.fit( train_loader, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=valid_loader, callbacks=callbacks, ) . where . steps_per_epoch = ( int( ( len(train0_dataset[0]) + len(train1_dataset[0]) + len(train2_dataset[0]) + len(train3_dataset[0]) + len(train4_dataset[0]) ) / batch_size ) + 1 ) . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2022/08/03/segmentation-model-part2.html",
            "relUrl": "/2022/08/03/segmentation-model-part2.html",
            "date": " • Aug 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Segmentation Model-Part I - Training deep learning segmentation models in Tensorflow",
            "content": "In this post, we will cover how to train a segmentation model by using the TensorFlow platform . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks, each folder has four sub-folders 1, 2, 3, 4 corresponds to four types of distribution of nails. Images is the data folder and Masks is the label folder, which is the segmentations of input images. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . For the convenience of loading data, we will store data information in the data frame (or CSV file). . We want to have the CSV file that stores the image and mask paths . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . To do that we use . import os from typing import Any import pandas as pd from utils import get_all_items, mkdir def make_csv_file(data_root) -&gt; None: list_images_train_masks = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;masks&quot;)) list_images_train_images = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;images&quot;)) list_images_train = [ i for i in list_images_train_images if i in list_images_train_masks ] print(len(list_images_train)) list_images_valid = get_all_items(os.path.join(data_root, &quot;valid&quot;, &quot;masks&quot;)) train_frame = pd.DataFrame(list_images_train, columns=[&quot;images&quot;]) train_frame[&quot;train&quot;] = 1 valid_frame = pd.DataFrame(list_images_valid, columns=[&quot;images&quot;]) valid_frame[&quot;train&quot;] = 0 mkdir(f&quot;{data_root}/csv_file&quot;) train_frame.to_csv(f&quot;{data_root}/csv_file/train.csv&quot;, index=False) valid_frame.to_csv(f&quot;{data_root}/csv_file/valid.csv&quot;, index=False) . Here get_all_items, mkdir are two supported functions (defined in utils.py file) that help us to find all items in a given folder and make a new folder. . Once we have the data frame, we can go to define the dataset. . 3. Define DataLoader . In this part we will do the following: . Get lists of images and masks | Define Dataloader with input being a list of images and masks and output be list of image batchs which being fed into the model. More precisely: Decode images and masks (read images and masks) | Transform data | Batch the augmented data. | . | . Before going to the next part, let’s talk about the advantages of using tf.data for the data loader pipeline. . The main feature of the next part is the data loader. We use the tensorflow.data (tf.data) to load the dataset instead of using Sequence Keras (keras.Sequence). In fact, we can also combine tf.data and keras.Sequence. This tutorial focuses on how to load data by tf.data. . Here is the pipeline loader of tf.data: . Read data from a CSV file | Transfrom (augumentate) the data | Load data into the model | . . The advantage of this method is: . Loading data by using multi-processing | Don’t have the memory leak phenomenal | Flexible to load dataset, can load weight sample data (using tf.compat.v1.data.experimental.sample_from_datasets ) | Downtime and waiting around are minimized while processing is maximized through parallel execution; see the following images: | . Naive pipeline . . This is the typical workflow of a naive data pipeline, there is always some idle time and overhead due to the inefficiency of sequential execution. . In contrast, consider: tf.data pipeline . . 3.1 Get images and masks from a dataframe. . def load_data_path(data_root: Union[str, Path], csv_dir: Union[str, Path], train: str) -&gt; Tuple: csv_file = pd.read_csv(csv_dir) ids = sorted(csv_file[&quot;images&quot;]) images = [data_root + f&quot;/{train}/images&quot; + fname for fname in ids] masks = [data_root + f&quot;/{train}/masks&quot; + fname for fname in ids] return (images, masks) . 3.2 Decode images and masks . def load_image_and_mask_from_path(image_path: tf.string, mask_path: tf.string) -&gt; Any: &quot;&quot;&quot;this function is to load image and mask Args: image_path (tf.string): the tensorflow string of image mask_path (tf.string): the tensorflow string of mask Returns: [type]: image and mask &quot;&quot;&quot; # read image by tensorflow function img = tf.io.read_file(image_path) img = tf.image.decode_image(img, channels=3) # read mask by tensorflow function mask = tf.io.read_file(mask_path) mask = tf.image.decode_image(mask, channels=1) return img, mask . 3.3 Doing augmentation . def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask . Here we use Albumentations library to define the transform. Albumentations is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance and does so while providing a concise yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection. For example, we define our validation transform as . import albumentations as A def valid_transform(): return A.Compose( [ A.Resize(384, 384, always_apply=True), ], p=1, ) . You can find the detail of transforms in transform.py file, in the source code given at the post’s end. We remark that, after doing augmentation, we cast the output of transform into TensorFlow type tensorflow type . aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) . Once we finish the augmentation task, we can do batching of the data by . dataset = dataset.batch(batch_size) . Here, the dataset is now an object of tf.data. . Compose four previous steps, we have the data loader function: . def tf_dataset( dataset: Tuple[List[str], List[str]], shuffle: bool, batch_size: Any, transforms: A.Compose, dtype: Any, device: List[int], ): r&quot;&quot;&quot;This function is to create dataloader for tensorflow training Args: dataset Tuple[List[str], List[str]]: Tuple of List data path that have same size shuffle (bool): True if you want shuffle dataset when do training batch_size [Any]: None if you dont want spit dataset by batch transforms (A.Compose): the augumentation that you want to apple for the data Returns: datast : the prepare dataset for the training step &quot;&quot;&quot; # do augumentation by albumentations, remark that in the the end, we use tf.cast to normalize # image and mask and also make sure that the output of this function be in form of tensorflow (tf) def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask def process_data(image, mask): # using tf.numpy_function to apply the aug_img to image and mask aug_img, aug_mask = tf.numpy_function(aug_fn, [image, mask], [dtype, dtype]) return aug_img, aug_mask # convert the tuple of list (images, masks) into the tensorflow.data form dataset = tf.data.Dataset.from_tensor_slices(dataset) # apply the map reading image and mask (make sure that the input and output are in the tensorflow form (tf.)) dataset = dataset.map(load_image_and_mask_from_path, num_parallel_calls=multiprocessing.cpu_count() // len(device)) # shuffle data if shuffle: dataset = dataset.shuffle(buffer_size=100000) # do the process_data map (augumentation and normalization) dataset = dataset.map( partial(process_data), num_parallel_calls=multiprocessing.cpu_count() // len(device) ).prefetch(AUTOTUNE) # make batchsize, here we use batch_size as a parameter, in some case we dont split dataset by batchsize # for example, if we want to mix multi-dataset, then we skip this step and split dataset by batch_size later if batch_size: dataset = dataset.batch(batch_size) return dataset . 4. Define the Segmentation model . In this part, we will define the segmentation model by using segmentation_models library, we also define the loss function, optimization, and metrics. . Segmentation models is a python library with Neural Networks for Image Segmentation based on Keras (Tensorflow) framework. This is the high-level API, you need only some lines of code to create a Segmentation Neural Network. . 4.1 Model . def create_model(): model = sm.Unet( &quot;efficientnetb4&quot;, input_shape=(384, 384, 3), encoder_weights=&quot;imagenet&quot;, classes=1, ) # TO USE mixed_precision, HERE WE USE SMALL TRICK, REMOVE THE LAST LAYER AND ADD # THE ACTIVATION SIGMOID WITH THE DTYPE TF.FLOAT32 last_layer = tf.keras.layers.Activation(activation=&quot;sigmoid&quot;, dtype=tf.float32)(model.layers[-2].output) model = tf.keras.Model(model.input, last_layer) # define optimization, here we use the tensorflow addon, but use can also use some normal # optimazation that is defined in tensorflow.optimizers optimizer = tfa.optimizers.RectifiedAdam() if args.mixed_precision: optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer, dynamic=True) # define a loss fucntion dice_loss = sm.losses.DiceLoss() focal_loss = sm.losses.BinaryFocalLoss() total_loss = dice_loss + focal_loss # define metric metrics = [ sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), ] # compile model with optimizer, losses and metrics model.compile(optimizer, total_loss, metrics) return model . Here we use: . The Unet model with the backbone is efficientnetb4 | The loss function is the sum DiceLoss and FocalLoss | The metric is IOU score and FSscore | The optimization algorithm is RectifiedAdam | . 5 Model Training . Once we have: dataloader and model we then combine them to run the model. In this part we will introduce some tools that help us boost the efficiency of training: . mixed_precision | using wanbd as a callback | . 5.1 Mixed_precision . How does mixed precision work? . Mixed precision training is the use of lower-precision operations (float16 and bfloat16) in a model during training to make it run faster and use less memory. Using mixed precision can improve performance by more than 3 times on modern GPUs and 60% on TPUs. . Here is the mixed precision training flow: . . We first feed the data as the float16 or bloat16 type, then the input of the model has the low type (float16 and bfloat16). | All of the calculations in the model are computed with the lower-precision operations | Convert the output of the model into float32 to do optimization tasks. | Update weights, convert them into lower-precision, and continue the next round of training. | . To train the model in TensorFlow with mixed precision, we just modify: . We first define the global policy: | . if args.mixed_precision: policy = mixed_precision.Policy(&quot;mixed_float16&quot;) mixed_precision.set_policy(policy) print(&quot;Mixed precision enabled&quot;) . Change the out data (input of model) into tf.float16: | . When we load dataset, before do suffling and do batching we convert out data into float16. To do that, . def process_data(image, mask): # using tf.numpy_function to apply the aug_img to image and mask aug_img, aug_mask = tf.numpy_function(aug_fn, [image, mask], [dtype, dtype]) return aug_img, aug_mask . Fix the last layer of the model. Here we remark that the dtype of the last layer should be float32. To do that, in the model part, we add some trick lines: | . model = sm.Unet( &quot;efficientnetb4&quot;, input_shape=(384, 384, 3), encoder_weights=&quot;imagenet&quot;, classes=1, ) # TO USE mixed_precision, HERE WE USE SMALL TRICK, REMOVE THE LAST LAYER AND ADD # THE ACTIVATION SIGMOID WITH THE DTYPE TF.FLOAT32 last_layer = tf.keras.layers.Activation(activation=&quot;sigmoid&quot;, dtype=tf.float32)( model.layers[-2].output ) . 5.2 Using Wanbd for logging. . In this part, we will cover how to use wandb for logging. WandB is a central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live and share your findings. To do that we use callback of model training as the WandbLogging . import wandb from wandb.keras import WandbCallback logdir = f&quot;{work_dir}/tensorflow/logs/wandb&quot; mkdir(logdir) wandb.init(project=&quot;Segmentation by Tensorflow&quot;, dir=logdir) wandb.config = { &quot;learning_rate&quot;: earning_rate, &quot;epochs&quot;: epochs, &quot;batch_size&quot;: batch_size, } callbacks.append(WandbCallback()) . We finish the training task by calling the train loader and the valid loader and fitting the model. Then . 5.3 Dataloader . data_root = str(args.data_root) train_csv_dir = f&quot;{data_root}/csv_file/train.csv&quot; valid_csv_dir = f&quot;{data_root}/csv_file/valid.csv&quot; # set batch_size batch_size = args.batch_size epochs = args.epochs # get training and validation set train_dataset = load_data_path(data_root, train_csv_dir, &quot;train&quot;) train_loader = tf_dataset( dataset=train_dataset, shuffle=True, batch_size=batch_size, transforms=train_transform(), dtype=dtype, device=args.device, ) valid_dataset = load_data_path(data_root, valid_csv_dir, &quot;valid&quot;) valid_loader = tf_dataset( dataset=valid_dataset, shuffle=False, batch_size=batch_size, transforms=valid_transform(), dtype=dtype, device=args.device, ) . 5.4 Fit training . history = model.fit( train_loader, steps_per_epoch=total_steps, epochs=epochs, validation_data=valid_loader, callbacks=callbacks, ) . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2022/08/02/segmentation-model-part1.html",
            "relUrl": "/2022/08/02/segmentation-model-part1.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Hướng dẫn sủ dụng gói thư viện BeautifulSoup trong Python để  Web Scraping",
            "content": "Nội dung của bài viết nhằm hướng dẫn các bạn mới làm quen với đào ảnh từ internet bằng gói thư viện beautiful soup. . Bài viết gồm các ý chính chính sau: . Outline . 1. Giới thiệu về Web Scaping . | 2. Tổng quan cấu trúc web 2.1. HTML, CSS | 2.2. Cấu trúc web | . | 3. Bài toán cụ thể: craping ảnh từ trang web: http://www.globalskinatlas.com/diagindex.cfm 3.1. Cấu trúc của trang web | 3.2. Cách download ảnh thủ công | 3.3. Xây dựng thuật toán từ thác tác down ảnh thủ công | . | 4. Giới thiệu gói thư viện beautiful soup | 5. Hoàn thành code | . 1.Giới thiệu về Web Scaping . Internet có nguồn dữ liệu khổng lồ, dữ liệu mà chúng ta hoàn toàn có thể truy cập bằng cách sử dụng web cùng một công cụ lập trình (Python, C++). Web Scaping là tác vụ download tất cả thông tin liên quan từ một trang web cố định. Ví dụ chúng ta muốn download tất cả các ảnh từ trang web http://www.globalskinatlas.com/diagindex.cfm để làm phong phú kho dữ liệu. . Một số trang web cung cấp cho chúng ta thông qua một API (Application Programming Interface), một số trang web khác có thể co ngừoi dùng lấy dữ liệu thông qua database có sẵn. Ví dụ khi bạn muốn download ảnh từ một trang web, bạn click vào ảnh trên website, từ website sẽ đưa bạn tới một trang web khác, nơi đó có lưu trữ ảnh trực tiếp trên server. . 2. Tổng quan cấu trúc web . Trước khi đi sâu vào làm sao có thể download tất cả dữ liệu từ một trang web, chúng ta sẽ tìm hiểu cấu trúc của một trang web. Việc này giống như đi câu cá, bạn tìm hiểu cấu trúc của hồ nước, để có thêm thông tin giúp việc câu cá dễ dàng hơn. . 2.1 Tổng quan HTML, CSS . Khi chúng ta truy cập một trang web, trình duyệt web (Firefox, Chrome) đưa ra yêu cầu đến máy chủ của trang web. Yêu cầu này được gọi là yêu cầu GET, sau đó chúng ta nhận được thông tin từ máy chủ. Nguồn thông tin từ máy chủ sẽ vẫn được trả lại thông tin gồm những tập file. Nhờ trình duyệt web, các tập này sẽ hiển thị dứoi dạng web. Cấu thành của tập để trình duyệt web có thể đọc một trang web bao gồm: . HTML - nội dung chính của trang. CSS - File này hỗ trợ HTML để hiển thi web đẹp hơn. JS - Các tệp Javascript thêm tính tương tác cho các trang web. Hình ảnh - các định dạng hình ảnh, chẳng hạn như JPG và PNG, cho phép các trang web hiển thị hình ảnh. Sau khi trình duyệt của chúng tôi nhận được tất cả các tệp, nó sẽ hiển thị trang và hiển thị cho chúng tôi. . Ví dụ: Khi chúng ta vào trình duyệt Chrome, chúng ta muốn try tập vào trang http://www.globalskinatlas.com/diagindex.cfm , khi đó máy chủ sẽ trả lại một tập, tập dữ liệu này gồm các file (html, css, javascript,), các file này sẽ được gửi trực tiếp về Chrome, thông qua trình duyệt, tất cả các tệp này sẽ tạp nên một trang web. . Để hiểu rõ cấu trúc một trang web, chúng ta sẽ tìm hiểu sâu file HTML. Ở các trình duyệt. Để hiển thị cấu trúc file HTML, chúng ta bấm phím F12. . 2.2 Tổng quan HTML . Cấu trúc cơ bản của trang HTML có dạng như sau: . - &lt;!Doctype&gt;: Phần khai báo chuẩn của html hay xhtml. - &lt;head&gt;&lt;/head&gt;: Phần khai báo ban đầu, khai báo về meta, title, css, javascript… - &lt;body&gt;&lt;/body&gt;: Phần chứa nội dung của trang web, nơi hiển thị nội dung. . &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Phần tiêu đề của html &lt;/title&gt; &lt;/head&gt; &lt;body&gt; ...Phần thân của html ... &lt;/body&gt; &lt;/html&gt; . Ở phần tiếp theo chúng ta sẽ giới thiệu về thẻ liên kết &lt;a&gt;, một những phần quan trọng nhất để thực hành đào ảnh. . 2.2.1 Thẻ liên kết a . - Thẻ liên kết &lt;a &gt; &lt;/a&gt; dùng để tạo một liên kết từ trang web này sang trang web khác, từ vị trí này sang vị trí khác hay dùng để mở ra một object nào đó (có thể là file words, ảnh, excel, pdf, mp3, movie,...), thẻ này có một thuộc tính bắt buộc: - href: Chứa đường dẫn cụ thể tới mục tiêu liên kết. . Ví dụ: Trong trang web http://www.globalskinatlas.com/diagdetail.cfm?id=91, khi chúng ta sử dụng phím f12, một trong những tag &lt;a&gt; &lt;/a&gt; có dạng như sau . &lt;a href=&quot;imagedetail.cfm?TopLevelid=170&amp;amp;ImageID=462&amp;amp;did=8 &quot;&gt;View&lt;/a&gt; . - Bằng truy cập trang web, ta thấy được liên kết ở tag này là: http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=170&amp;ImageID=462&amp;did=8 - Text để mô tả tag này là *View* . 2.2.2 Thẻ liên kết img . - Thẻ hiển thị một image &lt;*img*/&gt; dùng để nhúng một ảnh thông qua thuộc tính src, thẻ này có 2 thuộc tính bắt buộc: - src: Chứa đường dẫn tham chiếu tới image. - alt: Được sử dụng như một văn bản thay thế khi image không hiển thị (hoặc không có sẵn). . Cấu trúc của thẻ &lt;img&gt; không có sử dụng thẻ đóng (không dùng &lt;img&gt;&lt;/img&gt;), mà sử dụng ký tự kết thúc là một khoảng trắng và ký tự “/”. Tham khảo thêm về thẻ &lt;img/&gt;. . 3.Bài toán cụ thể: craping ảnh từ trang web: http://www.globalskinatlas.com/diagindex.cfm . Ở phần này chúng ta sẽ đi sâu vào phân tích cụ thể và định hướng hướng làm. . Đầu bài: Download tất cả ảnh có ở trang web http://www.globalskinatlas.com/diagindex.cfm và lưu trữ ảnh đó ở thư mục phù hợp. . Nhiệm vụ xuất phát của bài toán xuất phát từ nhu cầu thu thập ảnh bệnh nhân bị bệnh và chúng ta muốn lưu trữ thông tin các bệnh của từng ảnh để tiện sau này sử dụng cho các mô hình về machine learning. . 3.1 Cấu trúc của trang web . Chúng ta cùng xem xét cấu trúc của trang web. Khi thực hiện vào trang web, màn hình sẽ hiện thị như ảnh bên dưới. Chúng ta có thể thấy file được có rất nhiều tag &lt;a&gt;&lt;/a&gt;, mỗi tag tương ứng với một bệnh. . . Bấm phím f12 để thấy được cấu trúc của trang web. Mỗi tag bệnh sẽ tương ứng với . &lt;a href=&quot;diagdetail.cfm?id=653&quot;&gt;&lt;/a&gt; . Nếu click vào một tag, sẽ đưa chúng ta tới một trang web mới, ví dụ ở đây chúng ta click vào Ecthyma, chúng ta được tới trang web http://www.globalskinatlas.com/diagdetail.cfm?id=653 . . Nhận thấy rằng diagdetail.cfm?id=653 sẽ là tương ứng với bệnh Ezthyma. Và id 653 sẽ tương ứng với mã bệnh Ecthyma. Dispay của trang web sẽ có dạng hình như sau . . Khi click vào View ở góc cuối cùng, chúng ta sẽ được tới một trang web mới : http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6 . Trang web mới này được gắn ở href của một tag &lt;a&gt;&lt;/a&gt; của trang web http://www.globalskinatlas.com/diagdetail.cfm?id=653 , cụ thể nội dung của tag &lt;a&gt;&lt;/a&gt;: . &lt;a href=&quot;imagedetail.cfm?TopLevelid=1099&amp;amp;ImageID=2615&amp;amp;did=6&quot;&gt;View&lt;/a&gt; . Có thể thấy “imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6” là tag gắn liền với trang web http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6. . Hiện thị của trang web sẽ có dạng như sau . . Ở trang web này, khi nhấp chuột phải, chúng ta hoàn toàn có thể download ảnh thủ công. Nhưng khi tiến hành download ảnh, chúng ta nhận ra rằng, chỉ có ảnh ở trung tâm có size ảnh lớn, những ảnh nhỏ hơn sẽ có size nhỏ hơn. Vì vậy chúng ta sẽ tiến hành download ảnh ở trung tâm, còn với mỗi ảnh nhỏ ở dưới, chúng ta sẽ nhấn click chuột vào ảnh, ví dụ chúng ta sẽ click vào ảnh nhỏ đầu tiên, từ link ảnh nhỏ sẽ đưa tới trang web có display như sau . . Chúng ta sẽ tiếp tục download ảnh ở trung tâm và lưu ở thư mục bệnh. . 3.2 Thuật toán đào ảnh thủ công . Dựa những phân tích như ở trên, chúng ta xây dựng được thuật toán đào ảnh như sau . Truy cập trang web http://www.globalskinatlas.com/diagindex.cfm 1.1 Truy cập tag bệnh đầu tiên , tạo thư mục bệnh đầu tiên 1.1.1 Truy vào tag View đầu tiên | . | . 1.1.1.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.1.1.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.1.1.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - 1.1.2 Truy vào tag View thứ hia | 1.1.2.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.1.2.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.1.2.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - … - 1.2 Truy cập tag bệnh đầu tiên , tạo thư mục bệnh đầu tiên - 1.2.1 Truy vào tag View đầu tiên | 1.2.1.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.2.1.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.2.1.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - 1.2.2 Truy vào tag View thứ hia | 1.2.2.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.2.2.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.2.2.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - … | . | | . Thuật toán trên đảm bảo giúp chúng ta có thể tải tất cả các ảnh và lưu trữ ảnh ở thư mục phù hợp. Nhiệm vụ của chúng ta bước tiếp theo sẽ tìm hiểu gói thưu viện beautiful soup và thực hiện hoá thuật toán thủ công . 4 Giới thiệu gói thư viện beautiful soup . BeautifulSoup là một gói thư viện của Python nhằm giúp người dùng dễ dàng lấy dữ liệu ra khỏi các file HTML và XML. Bạn đọc có thể tham khảo chi tiết ở trang web https://www.howkteam.vn/d/thu-vien-beautiful-soup-460. . Trong khuôn khô bài viết, chúng ta sẽ tìm hiểu những lệnh cơ bản sau: . Lệnh khởi tạo soup . r = requests.get(web_url) soup = BeautifulSoup(r.content, &quot;html.parser&quot;) . Lệnh lấy tất cả &lt;a&gt; tag . Lệnh trên nhằm tạo món soup dựa trên nguyên liệu trang web web_url, giúp bạn dễ dàng hơn trong việc truy cập dữ liệu của file HTML web_url . links = soup.findall(&quot;a&quot;, href = Trues) . Lấy url ở trong một tag . Lệnh giúp bạn lấy url trong một tag &lt;a&gt;&lt;/a&gt; từ món soup có sẵn. . html_doc = &quot;&quot;&quot; &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &quot;&quot;&quot; from bs4 import BeautifulSoup soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) for a in soup.find_all(&#39;a&#39;, href=True): print (&quot;Found the URL:&quot;, a[&#39;href&#39;]) . Out put trả ra: . Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh . Lấy text ở trong một tag . Để lấy một tag trong một tag từ soup, chúng ta sử dụng lệnh: . tag.text.strip() . Ví dụ . html_doc = &quot;&quot;&quot; &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &quot;&quot;&quot; from bs4 import BeautifulSoup soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) for a in soup.find_all(&#39;a&#39;, href=True): print (&quot;Found the text :&quot;, a.text.strip()) . 5. Hoàn thành code craping dữ liệu cho bài toán . Từ những lệnh cở bản của BeautifulSoup ở mục 4 và thuật toán được xây dựng ở mục 3. Chúng ta xây dựng hàm trên python để craping toàn bộ ảnh bệnh từ trang web http://www.globalskinatlas.com/. . Source code ở link .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2021/11/15/web-scraping.html",
            "relUrl": "/2021/11/15/web-scraping.html",
            "date": " • Nov 15, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hphuongdhsp.github.io/ml-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hphuongdhsp.github.io/ml-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I’am Hoang-Phuong. . I am currently working at MedTech Torus-Actions as a Data Scientist. . My passion is in several domains: Machine Learning, Cryptocurrency, Fintech. I like to follow the latest technology in Deep Learning. . Beside works, I like play sports: football, billard, pingpong,… .",
          "url": "https://hphuongdhsp.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hphuongdhsp.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}