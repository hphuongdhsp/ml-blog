{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation Model-Part VI -  Training the Segformer model by using Pytorch Lightning and HuggingFace\n",
    "> The sixth part of the Segmentation Tutorial Series, a step-by-step guide to developing Instance Segmentation Models in MMDetection\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- author: Nguyen Hoang-Phuong\n",
    "- categories: [jupyter]\n",
    "- image: _notebooks/my_icons/segformer_architecture.png\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This post is a demonstration of using the Segformer model in HuggingFace. We will focus on:\n",
    "\n",
    "- Architecture of the Segformer model\n",
    "- Traing the Segformer model by using Pytorch Lightning and HuggingFace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous post, we will work with the Segmentation Problem (Nail Segmentation). In the first and second parts we will recall `Problem Description and Dataset`. If you have followed previous posts, you can skip those parts. In the third part, we will focus on the advantages of the Segformer model. The last part of the post will cover the training the Segformer model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image.\n",
    "\n",
    "\n",
    "|                                                     Images                                                     |                                                     Masks                                                      |\n",
    "| :------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------: |\n",
    "| <img align=\"center\" width=\"300\"  src=\"https://habrastorage.org/webt/em/og/9v/emog9v4ya7ssllg5dht77_wehqk.png\"> | <img align=\"center\" width=\"300\"  src=\"https://habrastorage.org/webt/hl/bf/ov/hlbfovx1uhrbbebgxndyho9yywo.png\"> |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Our data is organized as\n",
    "\n",
    "```bash\n",
    "├── Images\n",
    "│   ├── 1\n",
    "│       ├── first_image.png\n",
    "│       ├── second_image.png\n",
    "│       ├── third_image.png\n",
    "│   ├── 2\n",
    "│   ├── 3\n",
    "│   ├── 4\n",
    "├── Masks\n",
    "│   ├── 1\n",
    "│       ├── first_image.png\n",
    "│       ├── second_image.png\n",
    "│       ├── third_image.png\n",
    "│   ├── 2\n",
    "│   ├── 3\n",
    "│   ├── 4\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We have two folders: `Images` and `Masks`. `Images` is the data folder, and `Masks` is the label folder, which is the segmentations of input images. Each folder has four sub-folder:  `1`, `2`, `3`, and `4`, corresponding to four types of nail distribution.\n",
    "\n",
    "We download data from [link](https://drive.google.com/file/d/1qBLwdQeu9nvTw70E46XNXMciB0aKsM7r/view?usp=sharing) and put it in `data_root`, for example\n",
    "\n",
    "```python\n",
    "data_root = \"./nail-segmentation-dataset\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Preparation\n",
    "\n",
    "Similar to the training pipeline of the previous post, we first make the data frame to store images and masks infos. \n",
    "\n",
    "| index | images                |\n",
    "| ----- | --------------------- |\n",
    "| 1     | path_first_image.png  |\n",
    "| 2     | path_second_image.png |\n",
    "| 3     | path_third_image.png  |\n",
    "| 4     | path_fourth_image.png |\n",
    "\n",
    "For that we use `make_csv_file` function in `data_processing.py` file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Segformer Model for the semanctic segmentation problem\n",
    "\n",
    "The SegFormer model was proposed in SegFormer: [Simple and Efficient Design for Semantic Segmentation with Transformers](https://arxiv.org/pdf/2105.15203v1.pdf) by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on image segmentation benchmarks. \n",
    "\n",
    "The figure below illustrates the architecture of SegFormer\n",
    "\n",
    "![](https://habrastorage.org/webt/rj/pf/lv/rjpflvzjcjdeh7vxnls2lzzfl38.png \"the architecture of SegFormer\")\n",
    "\n",
    "<!-- <img align=\"center\" width=\"600\"  src=\"https://habrastorage.org/webt/rj/pf/lv/rjpflvzjcjdeh7vxnls2lzzfl38.png\"> -->\n",
    "\n",
    "SegFormer has the following notable points:\n",
    "\n",
    "- The new Transformer encoder (backbone): Mix Transformer (MiT) that extracts coarse and fine features\n",
    "- The decoder is a MLP network to directly fuse the multi-level\n",
    "features of the encoder part and predicts the semantic segmentation mask\n",
    "\n",
    "### 3.1 Encoder\n",
    "\n",
    "The encoder of SegFormer is a Mix Transformer(MiT). There are six versions of encoders: MiT-B0 to MiT-B5. They have the same architecture, but different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest model for the best performance. The design of MiT is similar to the Vison Transformer, but it is modified to adapt with the semantic segmentation, namely, \n",
    "\n",
    "- **Hierarchical Feature Representation**: Unlike ViT that can only generate a single-resolution feature\n",
    "map, MiT generate multi-level features to adapt with the semantic segmentation. We can see the multi-level features idea is one of the most important ideas for the semantic segmentation, for example: HRNET, PSPNet, DeepLab, FPN, ...\n",
    "- **Overlapped Patch Merging**: In Vision Transformer, a image input is splitted into **partition** patches. With the Mix Transformer, a image input is also splitted into patches, but there are overlapping.\n",
    "\n",
    "```python\n",
    "\n",
    "class OverlapPatchMerging(nn.Sequential):\n",
    "    def __init__(\n",
    "        self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int\n",
    "    ):\n",
    "        super().__init__(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                kernel_size=patch_size,\n",
    "                stride=overlap_size,\n",
    "                padding=patch_size // 2,\n",
    "                bias=False\n",
    "            ),\n",
    "            LayerNorm2d(out_channels)\n",
    "        )\n",
    "\n",
    "```\n",
    "\n",
    "|                                                     Partition Patch                                                     |                                                     Overlapped Patch                                                      |\n",
    "| :------------------------------------------------------------------------------------------------------------: | :------------------------------------------------------------------------------------------------------------: |\n",
    "| <img align=\"center\" width=\"300\"  src=\"https://habrastorage.org/webt/en/6i/5b/en6i5bbonwp5cpva70awswkefie.png\"> | <img align=\"center\" width=\"300\"  src=\"https://habrastorage.org/webt/xj/gu/ps/xjgupsjrgxbxrtaqktzsia7ovsm.png\"> |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Efficient Self-Attention**: The main computation bottleneck of the encoders is the self-attention layer. The `Efficient Self-Attention` is implemented as the following:\n",
    "\n",
    "```python\n",
    "class EfficientMultiHeadAttention(nn.Module):\n",
    "    def __init__(self, channels: int, reduction_ratio: int = 1, num_heads: int = 8):\n",
    "        super().__init__()\n",
    "        self.reducer = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                channels, channels, kernel_size=reduction_ratio, stride=reduction_ratio\n",
    "            ),\n",
    "            LayerNorm2d(channels),\n",
    "        )\n",
    "        self.att = nn.MultiheadAttention(\n",
    "            channels, num_heads=num_heads, batch_first=True\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, _, h, w = x.shape\n",
    "        reduced_x = self.reducer(x)\n",
    "        # attention needs tensor of shape (batch, sequence_length, channels)\n",
    "        reduced_x = rearrange(reduced_x, \"b c h w -> b (h w) c\")\n",
    "        x = rearrange(x, \"b c h w -> b (h w) c\")\n",
    "        out = self.att(x, reduced_x, reduced_x)[0]\n",
    "        # reshape it back to (batch, channels, height, width)\n",
    "        out = rearrange(out, \"b (h w) c -> b c h w\", h=h, w=w)\n",
    "        return out\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **Mix-FFN**: Authors don't use the  positional encoding (PE) to introduce the location information as in the \n",
    "ViT. That is from the argument that positional encoding is actually not necessary for semantic segmentation. One intorduces the `Mix-FFN` is defined as: \n",
    "\n",
    "$$x_{out} = MLP(GELU(CONV_{3 \\times 3}(MLP(x_{in})))) + x_{in}$$\n",
    "\n",
    "\n",
    "More precisely,\n",
    "```python\n",
    "class MixMLP(nn.Sequential):\n",
    "    def __init__(self, channels: int, expansion: int = 4):\n",
    "        super().__init__(\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels, channels, kernel_size=1),\n",
    "            # depth wise conv\n",
    "            nn.Conv2d(\n",
    "                channels,\n",
    "                channels * expansion,\n",
    "                kernel_size=3,\n",
    "                groups=channels,\n",
    "                padding=1,\n",
    "            ),\n",
    "            nn.GELU(),\n",
    "            # dense layer\n",
    "            nn.Conv2d(channels * expansion, channels, kernel_size=1),\n",
    "        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decoder\n",
    "\n",
    "The Mix Transformer do well for the encoder part, then for the decoder part, we use All-MLP to fuse the multi-level features of the encoder part. \n",
    "\n",
    "<img align=\"center\" width=\"500\"  src=\"https://habrastorage.org/webt/jg/-d/29/jg-d29v79uubc9mh2djmt12xzvs.png\">\n",
    "\n",
    "Each Block of MLP-ll has the following form:\n",
    "```python\n",
    "class SegFormerDecoderBlock(nn.Sequential):\n",
    "    def __init__(self, in_channels: int, out_channels: int, scale_factor: int = 2):\n",
    "        super().__init__(\n",
    "            nn.UpsamplingBilinear2d(scale_factor=scale_factor),\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1),\n",
    "        )\n",
    "```\n",
    "\n",
    "For the MLP-All. \n",
    "Now we can jump to the next part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Traing the Segformer model with Pytorch Lightning and HuggingFace. \n",
    "\n",
    "In this part we will discover how to train the Segformer model. In the [part III](https://hphuongdhsp.github.io/ml-blog/2022/08/03/segmentation-model-part3.html), we have used the `segmentation_models_pytorch` to build a `Unet` model to deal with the nail the segmentation problem. Unfortunately, the `segmentation_models_pytorch` don't yet implement`SegFormer` model. There are some open sources that implement the `SegFormer` model:\n",
    "\n",
    " - [MMSegmentation](https://github.com/open-mmlab/mmsegmentation)\n",
    " - [Transformers - HuggingFace](https://github.com/huggingface/transformers)\n",
    " - [Implementing SegFormer in PyTorch](https://github.com/FrancescoSaverioZuppichini/SegFormer) \n",
    "The first one is the officinal source code, but the model sticks with the `MMSegmentation` platform. It will be difficulty for unfamiliar people to use the `MMSegmentation` platform. The third one is reimplemented, but the model is not trained in any data. So We cannot profit the pretrained weights. Then we choose the second one that is implemented and trained by the `HuggingFace` team. \n",
    "\n",
    "We will reuse the datapipeline and modelpipeline of the [third part](https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%203-Pytorch%20Lightning) of the tutorial series except that we will use the `transformer` library to build the Segformer model.\n",
    "\n",
    "```python\n",
    "from transformers import SegformerForSemanticSegmentation\n",
    "\n",
    "class SegFormer(nn.Module):\n",
    "    def __init__(\n",
    "        self, pretrained: str = \"nvidia/segformer-b4-finetuned-ade-512-512\", size: int = 512, num_labels: int = 9\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.segformer = SegformerForSemanticSegmentation.from_pretrained(\n",
    "            pretrained, ignore_mismatched_sizes=True, num_labels=num_labels\n",
    "        )\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.segformer(x)\n",
    "\n",
    "        upsampled_logits = torch.nn.functional.interpolate(\n",
    "            outputs.logits, size=(self.size, self.size), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        return upsampled_logits\n",
    "\n",
    "```\n",
    "\n",
    "Here we use the pretrained of `\"nvidia/segformer-b4-finetuned-ade-512-512\"`. It means that:\n",
    "\n",
    "- MiT-B4 Mix-Transformer is used to build the encoder part.\n",
    "- Weight is trained on the ADE 20K dataset.\n",
    "- Size of image = 512\n",
    "\n",
    "> Note that the output of the `SegFormer` model is (128,128). We the use the resize function `torch.nn.functional.interpolate`. We can totally replace the resize function with any other weighted function: `nn.ConvTranspose2d`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then can define the `model module` and `data module` as the same in the part III: \n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "\n",
    "model = SegFormer(config.model.encoder_name, config.model.size, config.model.classes)\n",
    "\n",
    "datamodule = NailSegmentation(\n",
    "    data_root=data_root,\n",
    "    csv_path=csv_path,\n",
    "    test_path=\"\",\n",
    "    batch_size=batch_size,\n",
    "    num_workers=4,\n",
    "    )\n",
    "\n",
    "model_lighning = LitNailSegmentation(model=model, learning_rate=config.training.learning_rate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And run the `Trainer` API\n",
    "\n",
    "```python\n",
    "\n",
    "trainer = Trainer(args)\n",
    "trainer.fit(\n",
    "    model=model_lighning,\n",
    "    datamodule=datamodule\n",
    ")\n",
    "```\n",
    "\n",
    "**We can find the full source code at [github](https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%206-Pytorch%20with%20Huggingface)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References:\n",
    "- [SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers](https://github.com/NVlabs/SegFormer)\n",
    "- [SegFormer](https://huggingface.co/docs/transformers/model_doc/segformer)\n",
    "- [Implementing SegFormer in PyTorch](https://towardsdatascience.com/implementing-segformer-in-pytorch-8f4705e2ed0e)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "visualization-curriculum-gF8wUgMm",
   "language": "python",
   "name": "visualization-curriculum-gf8wugmm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
