{
  
    
        "post0": {
            "title": "Segmentation Model-Part VII -  Training Instance Segmentation in MMDetection",
            "content": "In this post, we will cover how to train a instance segmentation model by using the MMDetection library. . 1. Semantic Segmentation vs Instance Segmentation . We first introduce about: Semantic image segmentation, Object detection, Semantic Image segmentation . A Semantic image segmentation marks all pixels belonging to that tag, but won’t define the boundaries of each object. | A Object detection does not segment the object, but define the location of each individual object instance with a box. | Combining the semantic segmentation with the object detection leads to a instance segmentation | . . Nowaday, to tackle the instance segmentation problem, one use uselly Mask R-CNN model which is presented by [K.He] and all. For more detail about Mask R-CNN model, we refer to read Everything about Mask R-CNN: A Beginner’s Guide artical. . Mask R-CNN is the state-of-the-art model for the Instance Segmentation with three outputs of the model: mask, classes and boundary box. . . 1. Problem Description and Dataset . We will cover the nail instance segmentation. We want to have a bounding box and segment each nail in the picture. It’s from the real application. We want to make a nail disease classification application. To do that, the first step is cropping nails in the given image. Then each cropping nail image will be fed in to the classification model. . For the semantic nail segmentation, we can segment the nail in iamges and then use post-processing to obtain the bounding box and segmentation of nails. That method does not work well in the case that the nails have overlapping. We then aproach the instance segmentation problem to tackle the difficulty. . Images Masks . | | . Mission: We want to have a bounding box and segmentation of each nail in the picture. . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . We now have only the semantic segmentation dataset. This part we will make the instance segmentation datset and save that data in the form coco. . 2.1 Make data frame . For convenient, we will save all of dataset information in the csv files: . images,masks,width,height . images masks width height . images/1/filename1.png | masks/1/filename1.png | 256 | 256 | . images/1/filename1.png | masks/1/filename1.png | 256 | 256 | . images/2/filename1.png | masks/2/filename1.png | 256 | 256 | . images/2/filename1.png | masks/2/filename1.png | 256 | 256 | . The function make_csv_file helps us do the above task. . To do that we use two functions png2numpy, make_csv_file_npy in data_processing.py file. . 2.2 Get coco annotation . We want to convert our semantic segmentation data into the instance segmentaion. One of the famous format to organize the instance segmentation data is COCO. . The coco annotation has the following format . { &quot;images&quot;: [images], &quot;annotations&quot;: [annotations], &quot;categories&quot;: [categories] } . Where: . “images” (type: [List[Dict]]) is the list of dictionaries, each dictionary has informations “id”: 100 The id of image | “file_name”: “train/images/1/image_100.png”, the path to get image | “width”: 1800, | “height”: 1626 | . | “annotations” is the list of dictionaries, each dictionary has informations . “id”: 350, id of object (not the image id) | “image_id”: 100, id of image | “category_id”: 1, id of categories | “segmentation”: RLE or [polygon], | “area”: float, | “bbox”: [x,y,width,height], | “iscrowd”: 0 or 1, | . | “categories” is the list of dictionaries, each dictionary has informations “id”: int = 0 id of categories | “name”: str = “nail” | . | . Using the get_annotations function, we can convert the semantic segmentation data into the coco format data of the instance segmentation. . def get_annotations(dataframe: pd.DataFrame): &quot;&quot;&quot;get_annotations is to convert a dataframe into the coco format Args: train_df (pd.DataFrame): the dataframe that stored the infomation of the dataset. the form of the dataframe is images | width | height | Returns: [type]: the coco format data of the dataset &quot;&quot;&quot; cats = [{&quot;id&quot;: 0, &quot;name&quot;: &quot;nail&quot;}] annotations = [] images = [] obj_count = 0 for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe)): filename = row.images images.append( { &quot;id&quot;: idx, &quot;file_name&quot;: filename, &quot;width&quot;: row.width, &quot;height&quot;: row.height, } ) binary_mask = read_mask(os.path.join(str(data_root), row.masks)) contours = find_contours(binary_mask) for contour in contours: xmin = int(np.min(contour[:, :, 0])) xmax = int(np.max(contour[:, :, 0])) ymin = int(np.min(contour[:, :, 1])) ymax = int(np.max(contour[:, :, 1])) poly = contour.flatten().tolist() poly = [x + 0.5 for x in poly] data_anno = { &quot;image_id&quot;: idx, &quot;id&quot;: obj_count, &quot;category_id&quot;: 0, &quot;bbox&quot;: [xmin, ymin, (xmax - xmin), (ymax - ymin)], &quot;area&quot;: (xmax - xmin) * (ymax - ymin), &quot;segmentation&quot;: [poly], &quot;iscrowd&quot;: 0, } if (xmax - xmin) * (ymax - ymin) &lt; 20: continue else: annotations.append(data_anno) obj_count += 1 return {&quot;categories&quot;: cats, &quot;images&quot;: images, &quot;annotations&quot;: annotations} . Where: . find_contours is a function to get contour of a binary mask. | dataframe argument of the above function is the data frame obtained from the make_csv_file that has the infomations of data. | . We then save the annotaions as a json file by the get_json_coco function . def get_json_coco(args) -&gt; None: train_df = pd.read_csv(f&quot;{data_root}/csv_file/train_info.csv&quot;) valid_df = pd.read_csv(f&quot;{data_root}/csv_file/valid_info.csv&quot;) coco_json = os.path.join(data_root, &quot;annotations&quot;) mkdir(coco_json) train_json = get_annotations(train_df) valid_json = get_annotations(valid_df) with open(f&quot;{coco_json}/train.json&quot;, &quot;w+&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(train_json, f, ensure_ascii=True, indent=4) with open(f&quot;{coco_json}/valid.json&quot;, &quot;w+&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(valid_json, f, ensure_ascii=True, indent=4) . For more details, we can find the source code at github . 3. Training instance segmentation problems by MMDetection . 3.1 MMDetection . MMDetection is an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. It is built on top of PyTorch. . One decomposes the detection framework into different components and one can easily construct a customized object detection framework by combining different modules. In this part, we discover how to decompose the instance segmentation framework and modify them in order to train a instance segmentation model. . To train a instance segmentation or object detection model, we pass to three steps: . Prepare the customized dataset | Prepare a config | Train, test, inference models on the customized dataset. | . In the second part we have customized our dataset into the coco format. With the coco format, we can easy reuse configurations. . 3.2 Modify the config. . Config is all we need . To run a instance segmentation or object detection, all we need to do is define a good config. In the config file, there are all of infomation for a training model. . Examples of configurations are given in config. There are a lot of configs that help to build a customized configs. For the convenience, we will download them and put them to the repository of MMDetection tutorial. . A Config can be decompose into four parts. . model: define the model architechture, loss function | dataset: define the data pipeline | schedules: define the optimization and the schedules learning rate | default_runtime: define the logging, check point. | . In the configs/__base__ there are examples for each module . ├── configs │ ├── __base__ │ ├── datasets │ ├── models │ ├── schedules │ ├── default_runtime.py . Also, inside of the configs, we have alot of subconfigs that coresponding to the model acrchitecture. . For example: . configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py . Here . mask_rcnn: type of mask_rcnn | r50: backbone of the model (Resnet50) | caffe: the pretrained model is caffe model. | fpn: the feature pyramid network. | mstrain: the multi-scale image for the data pipeline | poly: schedule poly | 1x: 12 max_epochs | coco: the dataset is coco format. | . In this post, we focus on two modules: dataset and model and set the schedules and default_runtime as default. . Modify the model config . With the nail segmentation, the output is a binary mask (only nail object), then redefine the model as: . # The new config inherits a base config to highlight the necessary modification _base_ = &quot;mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py&quot; # We also need to change the num_classes in head to match the dataset&#39;s annotation model = dict(roi_head=dict(bbox_head=dict(num_classes=1), mask_head=dict(num_classes=1))) . Here: . We inherit the config mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py | Only need to define the num_classes in the bbox_head and mask_head. | . Modify the data pipeline config . For the data pipeline: . data = dict( samples_per_gpu=2, workers_per_gpu=2, train=dict( type=&quot;CocoDataset&quot;, img_prefix=data_root, classes=cfg.classes, ann_file=f&quot;{data_root}/annotations/train.json&quot;, pipeline=cfg.train_pipeline, ), val=dict( type=&quot;CocoDataset&quot;, img_prefix=data_root, classes=cfg.classes, ann_file=f&quot;{data_root}/annotations/valid.json&quot;, pipeline=cfg.test_pipeline, ), test=dict( type=&quot;CocoDataset&quot;, img_prefix=data_root, classes=cfg.classes, ann_file=f&quot;{data_root}/annotations/valid.json&quot;, pipeline=cfg.test_pipeline, ), ) . Here: . type:”CocoDataset” as default because we use the coco format. | img_prefix: - the path to the image directory. | ann_file: the path to the json annotation file. | classes: the classes of the dataset. Here class: = [“nail”] | pipeline: data pipeline processing that is defined as | . train_pipeline = [ dict(type=&quot;LoadImageFromFile&quot;), dict(type=&quot;LoadAnnotations&quot;, with_bbox=True, with_mask=True), dict( type=&quot;Resize&quot;, img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)], multiscale_mode=&quot;value&quot;, keep_ratio=True, ), dict(type=&quot;RandomFlip&quot;, flip_ratio=0.5), dict(type=&quot;Normalize&quot;, **img_norm_cfg), dict(type=&quot;Pad&quot;, size_divisor=32), dict(type=&quot;DefaultFormatBundle&quot;), dict(type=&quot;Collect&quot;, keys=[&quot;img&quot;, &quot;gt_bboxes&quot;, &quot;gt_labels&quot;, &quot;gt_masks&quot;]), ] test_pipeline = [ dict(type=&quot;LoadImageFromFile&quot;), dict( type=&quot;MultiScaleFlipAug&quot;, img_scale=(1333, 800), flip=False, transforms=[ dict(type=&quot;Resize&quot;, keep_ratio=True), dict(type=&quot;RandomFlip&quot;), dict(type=&quot;Normalize&quot;, **img_norm_cfg), dict(type=&quot;Pad&quot;, size_divisor=32), dict(type=&quot;ImageToTensor&quot;, keys=[&quot;img&quot;]), dict(type=&quot;Collect&quot;, keys=[&quot;img&quot;]), ], ), ] . Note: we want use the multi-scale image when training the pipeline, then . img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)] . 3.2 Training . Once we have the config file (see [nail_conig.py], we start to train model. . For that we will: . import the config file | define the model module | defime the data pipeline | train the model with an api | . Import Config by using mmcv library: . cfg = mmcv.Config(&quot;configs/nail_config.py&quot;) . Build the model pipeline from the Config by using build_detector api . from mmdet.apis import build_detector model = build_detector(cfg.model, train_cfg=cfg.get(&quot;train_cfg&quot;), test_cfg=cfg.get(&quot;test_cfg&quot;)) model.init_weights() . Build the data pipeline from the Config by using build_dataset api . Using the apis: build_detector, build_dataset of mmdetection library, we can easily build the model and dataset. . from mmdet.apis import build_dataset datasets = [build_dataset(cfg.data.train)] . Train the model with the train_detector api . from mmdet.apis import train_detector train_detector(model, datasets) . After 40 epochs, we can see the model is training well. . . For more details, we can find the source code at github . References . Part 7-Instance Segmentation with MMDetection | MMDetection Tutorial - TRAIN WITH CUSTOMIZED DATASETS | .",
            "url": "https://hphuongdhsp.github.io/ml-blog/pytorchlightning/semanticsegmentation/deeplearning/maskrcnn/huggingface/2022/08/08/segmentation-model-part7.html",
            "relUrl": "/pytorchlightning/semanticsegmentation/deeplearning/maskrcnn/huggingface/2022/08/08/segmentation-model-part7.html",
            "date": " • Aug 8, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Segmentation Model-Part VI -  Training the Segformer model by using Pytorch Lightning and HuggingFace",
            "content": "This post is a demonstration of using the Segformer model in HuggingFace. We will focus on: . Architecture of the Segformer model | Traing the Segformer model by using Pytorch Lightning and HuggingFace. | . Similar to the previous post, we will work with the Segmentation Problem (Nail Segmentation). In the first and second parts we will recall Problem Description and Dataset. If you have followed previous posts, you can skip those parts. In the third part, we will focus on the advantages of the Segformer model. The last part of the post will cover the training the Segformer model. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . Similar to the training pipeline of the previous post, we first make the data frame to store images and masks infos. . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. . 3. The Segformer Model for the semanctic segmentation problem . The SegFormer model was proposed in SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers by Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, Ping Luo. The model consists of a hierarchical Transformer encoder and a lightweight all-MLP decode head to achieve great results on image segmentation benchmarks. . The figure below illustrates the architecture of SegFormer . . SegFormer has the following notable points: . The new Transformer encoder (backbone): Mix Transformer (MiT) that extracts coarse and fine features | The decoder is a MLP network to directly fuse the multi-level features of the encoder part and predicts the semantic segmentation mask | . 3.1 Encoder . The encoder of SegFormer is a Mix Transformer(MiT). There are six versions of encoders: MiT-B0 to MiT-B5. They have the same architecture, but different sizes. MiT-B0 is our lightweight model for fast inference, while MiT-B5 is the largest model for the best performance. The design of MiT is similar to the Vison Transformer, but it is modified to adapt with the semantic segmentation, namely, . Hierarchical Feature Representation: Unlike ViT that can only generate a single-resolution feature map, MiT generate multi-level features to adapt with the semantic segmentation. We can see the multi-level features idea is one of the most important ideas for the semantic segmentation, for example: HRNET, PSPNet, DeepLab, FPN, ... | Overlapped Patch Merging: In Vision Transformer, a image input is splitted into partition patches. With the Mix Transformer, a image input is also splitted into patches, but there are overlapping. | . class OverlapPatchMerging(nn.Sequential): def __init__( self, in_channels: int, out_channels: int, patch_size: int, overlap_size: int ): super().__init__( nn.Conv2d( in_channels, out_channels, kernel_size=patch_size, stride=overlap_size, padding=patch_size // 2, bias=False ), LayerNorm2d(out_channels) ) . Partition Patch Overlapped Patch . | | . Efficient Self-Attention: The main computation bottleneck of the encoders is the self-attention layer. The Efficient Self-Attention is implemented as the following: | . class EfficientMultiHeadAttention(nn.Module): def __init__(self, channels: int, reduction_ratio: int = 1, num_heads: int = 8): super().__init__() self.reducer = nn.Sequential( nn.Conv2d( channels, channels, kernel_size=reduction_ratio, stride=reduction_ratio ), LayerNorm2d(channels), ) self.att = nn.MultiheadAttention( channels, num_heads=num_heads, batch_first=True ) def forward(self, x): _, _, h, w = x.shape reduced_x = self.reducer(x) # attention needs tensor of shape (batch, sequence_length, channels) reduced_x = rearrange(reduced_x, &quot;b c h w -&gt; b (h w) c&quot;) x = rearrange(x, &quot;b c h w -&gt; b (h w) c&quot;) out = self.att(x, reduced_x, reduced_x)[0] # reshape it back to (batch, channels, height, width) out = rearrange(out, &quot;b (h w) c -&gt; b c h w&quot;, h=h, w=w) return out . Mix-FFN: Authors don&#39;t use the positional encoding (PE) to introduce the location information as in the ViT. That is from the argument that positional encoding is actually not necessary for semantic segmentation. One intorduces the Mix-FFN is defined as: | . $$x_{out} = MLP(GELU(CONV_{3 times 3}(MLP(x_{in})))) + x_{in}$$ . More precisely, . class MixMLP(nn.Sequential): def __init__(self, channels: int, expansion: int = 4): super().__init__( # dense layer nn.Conv2d(channels, channels, kernel_size=1), # depth wise conv nn.Conv2d( channels, channels * expansion, kernel_size=3, groups=channels, padding=1, ), nn.GELU(), # dense layer nn.Conv2d(channels * expansion, channels, kernel_size=1), ) . 3.2 Decoder . The Mix Transformer do well for the encoder part, then for the decoder part, we use All-MLP to fuse the multi-level features of the encoder part. . . Each Block of MLP-ll has the following form: . class SegFormerDecoderBlock(nn.Sequential): def __init__(self, in_channels: int, out_channels: int, scale_factor: int = 2): super().__init__( nn.UpsamplingBilinear2d(scale_factor=scale_factor), nn.Conv2d(in_channels, out_channels, kernel_size=1), ) . For the MLP-All. Now we can jump to the next part. . 4. Traing the Segformer model with Pytorch Lightning and HuggingFace. . In this part we will discover how to train the Segformer model. In the part III, we have used the segmentation_models_pytorch to build a Unet model to deal with the nail the segmentation problem. Unfortunately, the segmentation_models_pytorch don&#39;t yet implement SegFormer model. There are some open sources that implement the SegFormer model: . MMSegmentation | Transformers - HuggingFace | Implementing SegFormer in PyTorch The first one is the officinal source code, but the model sticks with the MMSegmentation platform. It will be difficulty for unfamiliar people of the MMSegmentation platform. The third one is reimplemented from scratch, but the model is not trained for any data. So we cannot profit the pretrained weights. We choose the second one that is implemented and trained by the HuggingFace team. | . We will reuse the datapipeline and modelpipeline of the third part of the tutorial series except that we will use the transformer library to build the Segformer model. . from transformers import SegformerForSemanticSegmentation class SegFormer(nn.Module): def __init__( self, pretrained: str = &quot;nvidia/segformer-b4-finetuned-ade-512-512&quot;, size: int = 512, num_labels: int = 9 ): super().__init__() self.segformer = SegformerForSemanticSegmentation.from_pretrained( pretrained, ignore_mismatched_sizes=True, num_labels=num_labels ) self.size = size def forward(self, x): outputs = self.segformer(x) upsampled_logits = torch.nn.functional.interpolate( outputs.logits, size=(self.size, self.size), mode=&quot;bilinear&quot;, align_corners=False ) return upsampled_logits . Here we use the pretrained of &quot;nvidia/segformer-b4-finetuned-ade-512-512&quot;. It means that: . MiT-B4 Mix-Transformer is used to build the encoder part. | Weight is trained on the ADE 20K dataset. | Size of image = 512Note that the output of the SegFormer model is (128,128). We the use the resize function torch.nn.functional.interpolate. We can totally replace the resize function with any other weighted function:nn.ConvTranspose2d. . | . We then can define the model module and data module as the same in the part III: . model = SegFormer(config.model.encoder_name, config.model.size, config.model.classes) datamodule = NailSegmentation( data_root=data_root, csv_path=csv_path, test_path=&quot;&quot;, batch_size=batch_size, num_workers=4, ) model_lighning = LitNailSegmentation(model=model, learning_rate=config.training.learning_rate) . And run the Trainer API . trainer = Trainer(args) trainer.fit( model=model_lighning, datamodule=datamodule ) . We can find the full source code at github . References: . SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers | SegFormer | Implementing SegFormer in PyTorch | .",
            "url": "https://hphuongdhsp.github.io/ml-blog/pytorchlightning/intancesegmentation/deeplearning/mmdetection/2022/08/07/segmentation-model-part6.html",
            "relUrl": "/pytorchlightning/intancesegmentation/deeplearning/mmdetection/2022/08/07/segmentation-model-part6.html",
            "date": " • Aug 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Segmentation Model-Part V - Data augmentation on the GPU with DALI",
            "content": "In the last post, we have discovered how to augmente data on GPUs with Kornia. This post, we will we discover how to use DALI to accelerate deep learning. . We will work with the Segmentation Problem (Nail Segmentation). For that, we use Pytorch Lightninig to train model and use DALI to load data and do data processing. In the first and second parts we will recall Problem Description and Dataset. If you have followed previous posts, you can skip that part. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . For the convenience of training with DALI, we will convert png data into npy format and save all of informations of images and masks in a CSV file . index images masks . 1 | path_first_image.npy | path_first_image.npy | . 2 | path_second_image.npy | path_second_image.npy | . 3 | path_third_image.npy | path_third_image.npy | . 4 | path_fourth_image.npy | path_fourth_image.npy | . To do that we use two functions png2numpy, make_csv_file_npy in data_processing.py file. . png2numpy function helps us convert the png format into the npy format and save images and masks in data_root_npy folder. | make_csv_file_npy makes 2 CVS files train.cvs and valid.csv having the previous form and be saved at f”{data_root_npy}/csv_file” folder. | . 3. NVIDIA Data Loading Library (DALI) . DALI is open source library for decoding and augmenting images,videos and speech to accelerate deep learning applications. DALI reduces latency and training time, mitigating bottlenecks, by overlapping training and pre-processing. It provides a drop-in replacement for built in data loaders and data iterators in popular deep learning frameworks for easy integration or retargeting to different frameworks. . Let us discuss the difference between a Naive Deeplearning Pipeline, Kornia Deep Learning Pipeline and DALI Deeplearning Pipeline. . Naive Deeplearning Pipeline: The pre-processing of the data occurs on the CPU, the model will be typically trained on GPU/TPU. . | Kornia Deep Learning Pipeline: The reading, resezing or padding data occur on CPU, the transform (augmentation) and model training ran on GPU/TPU. The transform is consider as an nn.Module. Then transform is the nn.Module object that forward input x of size $B times C times H times W$ and obtain the output of size $B times C times H times W$. . | . class ModelWithAugumentation(nn.Module): &quot;&quot;&quot;Module to perform data augmentation on torch tensors.&quot;&quot;&quot; def __init__(self, transform_module: nn.Module, model : nn.Module) -&gt; None: super().__init__() self.transform_module = transform_module self.model = model def forward(self, x: Tensor) -&gt; Tensor: augmented_x = self.transform_module(x) # BxCxHxW x_out = self.model(augmented_x) return x_out . DALI Deeplearning Pipeline: In the reading image, we have two components: encoding and decoding. With DALI library, we can read do encoding by CPUs and decoding by GPUs that work on batch. All other tasks will work on GPUs. We remark that the transform in DALI Pipeline works on data of several types: $B times C times H timesW$, $B times H times W timesC$. That is why DALI can easily be retargeted to TensorFlow, PyTorch, and MXNet. | . The DALI Training Pipeline . . DALI Library in the whole Pipieline. . . 4. Training the Segmentation problem with DALI and Pytorch Lighiting. . In this part, we will details how to do processing the data in DALI and train the model by Pytorch Lighiting. . 4.1 Define Data Pipeline by DALI . We will define the processing data pipeline by using dali, instead of using torch.utils.data. . We first define new class: GenericPipeline that wraps the nvidia.dali.pipeline.Pipeline class . import nvidia.dali.ops as ops import nvidia.dali.types as types from nvidia.dali.pipeline import Pipeline class GenericPipeline(Pipeline): def __init__(self, batch_size, num_threads, device_id, **kwargs): super().__init__(batch_size, num_threads, device_id) self.kwargs = kwargs self.dim = kwargs[&quot;dim&quot;] self.device = device_id # self.patch_size = [384,384] self.load_to_gpu = kwargs[&quot;load_to_gpu&quot;] self.input_x = self.get_reader(kwargs[&quot;imgs&quot;]) self.input_y = self.get_reader(kwargs[&quot;lbls&quot;]) self.cast = ops.Cast(device=&quot;gpu&quot;, dtype=types.DALIDataType.FLOAT) def get_reader(self, data): return ops.readers.Numpy( files=data, device=&quot;cpu&quot;, read_ahead=True, dont_use_mmap=True, pad_last_batch=True, shard_id=self.device, seed=self.kwargs[&quot;seed&quot;], num_shards=self.kwargs[&quot;gpus&quot;], shuffle_after_epoch=self.kwargs[&quot;shuffle&quot;], ) def load_data(self): img = self.input_x(name=&quot;ReaderX&quot;) # read X img = img.gpu() img = self.cast(img) if self.input_y is not None: lbl = self.input_y(name=&quot;ReaderY&quot;) # read Y lbl = lbl.gpu() lbl = self.cast(lbl) return img, lbl return img . The initial input of GenericPipeline is: . batch_size: batchsize | num_threads: number of workers | device_id: gpu device | kwargs: the dictionary that has the infomations of parameters and train/valid data. | . kwargs = { &quot;dim&quot;: 2, &quot;seed&quot;: 42, &quot;gpus&quot;: 1, &quot;overlap&quot;: 0.5, &quot;benchmark&quot;: False, &quot;num_workers&quot;: 4, &quot;oversampling&quot;: 0.4, &quot;test_batches&quot;: 0, &quot;train_batches&quot;: 0, &quot;load_to_gpu&quot;: True, } . We now can define ValidPipeline and TrainPipeline based on the GenericPipeline . ValidPipeline . class ValidPipeline(GenericPipeline): def __init__(self, batch_size, num_threads, device_id, **kwargs): super().__init__(batch_size, num_threads, device_id, **kwargs) # self.invert_resampled_y = kwargs[&quot;invert_resampled_y&quot;] # if self.invert_resampled_y: # self.input_meta = self.get_reader(kwargs[&quot;meta&quot;]) # self.input_orig_y = self.get_reader(kwargs[&quot;orig_lbl&quot;]) print(len(kwargs[&quot;imgs&quot;])) def define_graph(self): img, lbl = self.load_data() img, lbl = self.resize_fn(img, lbl) # reszie to inpput size (384) img, lbl = self.transpose_fn(img, lbl) return img, lbl . Here, we remark that define_graph is the function to do the data processing. For ValidPipeline, the flow of the data will be: . load_data | resize data | transpose data (convert image into the size of CxHxW) | . TrainPipeline . Similar, we have the TrainPipeline . class TrainPipeline(GenericPipeline): def __init__(self, batch_size, num_threads, device_id, **kwargs): super().__init__(batch_size, num_threads, device_id, **kwargs) self.oversampling = kwargs[&quot;oversampling&quot;] &quot;&quot;&quot; define some augumentations, for more augumentation, please read https://github.com/NVIDIA/DALI/tree/main/docs/examples/image_processing &quot;&quot;&quot; @staticmethod def slice_fn(img): return fn.slice(img, 1, 3, axes=[0]) def resize(self, data, interp_type): return fn.resize(data, interp_type=interp_type, size=self.crop_shape_float) def noise_fn(self, img): img_noised = img + fn.random.normal(img, stddev=fn.random.uniform(range=(0.0, 0.33))) return random_augmentation(0.15, img_noised, img) def blur_fn(self, img): img_blurred = fn.gaussian_blur(img, sigma=fn.random.uniform(range=(0.5, 1.5))) return random_augmentation(0.15, img_blurred, img) def brightness_fn(self, img): brightness_scale = random_augmentation(0.15, fn.random.uniform(range=(0.7, 1.3)), 1.0) return img * brightness_scale def contrast_fn(self, img): scale = random_augmentation(0.13, fn.random.uniform(range=(0.9, 1.1)), 1.0) return math.clamp(img * scale, fn.reductions.min(img), fn.reductions.max(img)) def flips_fn(self, img, lbl): kwargs = { &quot;horizontal&quot;: fn.random.coin_flip(probability=0.5), &quot;vertical&quot;: fn.random.coin_flip(probability=0.5), } return fn.flip(img, **kwargs), fn.flip(lbl, **kwargs) def define_graph(self): img, lbl = self.load_data() img, lbl = self.resize_fn(img, lbl) # reszie to inpput size (384) img, lbl = self.flips_fn(img, lbl) img = self.noise_fn(img) img = self.blur_fn(img) img = self.brightness_fn(img) img = self.contrast_fn(img) img, lbl = self.transpose_fn(img, lbl) return img, lbl . For the TrainPipeline, we need more augmentations. The augmentation is defined in the define_graph function. . For the augmentation in DALI, we need to redefine all of transform functions. For more infomation, read the dali tutorial. . For the convenience, we will define the function fetch_dali_loader that will generate Pipeline (TrainPipeline, ValidPipeline) depends on the type of dataset. . def fetch_dali_loader(imgs, lbls, batch_size, mode, **kwargs): assert len(imgs) &gt; 0, &quot;Empty list of images!&quot; if lbls is not None: assert len(imgs) == len(lbls), f&quot;Number of images ({len(imgs)}) not matching number of labels ({len(lbls)})&quot; pipeline = PIPELINES[mode] shuffle = True if mode == &quot;train&quot; else False load_to_gpu = True if mode in [&quot;eval&quot;, &quot;test&quot;] else False pipe_kwargs = {&quot;imgs&quot;: imgs, &quot;lbls&quot;: lbls, &quot;load_to_gpu&quot;: load_to_gpu, &quot;shuffle&quot;: shuffle, **kwargs} rank = int(os.getenv(&quot;LOCAL_RANK&quot;, &quot;0&quot;)) pipe = pipeline(batch_size, kwargs[&quot;num_workers&quot;], rank, **pipe_kwargs) return pipe . Once we have the TrainPipeline and ValidPipeline, we can use them for the LightningWrapper to make the lightning data module. . 4.2 LightningWrapper . Our LightningDataModule of Nail data is defined by . class NailSegmentationDaliDali(LightningDataModule): def __init__(self, data_root_npy: str, batch_size: int, csv_folder: str): super().__init__() self.data_root_npy = str(data_root_npy) self.csv_folder = csv_folder self.batch_size = batch_size self.train_csv = pd.read_csv(os.path.join(self.csv_folder, &quot;train.csv&quot;)) self.valid_csv = pd.read_csv(os.path.join(self.csv_folder, &quot;valid.csv&quot;)) self.kwargs = { &quot;dim&quot;: 2, &quot;seed&quot;: 42, &quot;gpus&quot;: 1, &quot;overlap&quot;: 0.5, &quot;benchmark&quot;: False, &quot;num_workers&quot;: 4, &quot;oversampling&quot;: 0.4, &quot;test_batches&quot;: 0, &quot;train_batches&quot;: 0, &quot;load_to_gpu&quot;: True, } def prepare_data(self) -&gt; None: pass def setup(self, stage=None): self.train_imgs = [os.path.join(self.data_root_npy, path) for path in self.train_csv[&quot;images&quot;]] self.train_lbls = [os.path.join(self.data_root_npy, path) for path in self.train_csv[&quot;masks&quot;]] self.val_imgs = [os.path.join(self.data_root_npy, path) for path in self.valid_csv[&quot;images&quot;]] self.val_lbls = [os.path.join(self.data_root_npy, path) for path in self.valid_csv[&quot;masks&quot;]] self.train_dataset = fetch_dali_loader( imgs=self.train_imgs, lbls=self.train_lbls, batch_size=self.batch_size, mode=&quot;train&quot;, **self.kwargs ) self.valid_dataset = fetch_dali_loader( imgs=self.val_imgs, lbls=self.val_lbls, batch_size=self.batch_size, mode=&quot;eval&quot;, **self.kwargs ) def train_dataloader(self): output_map = [&quot;image&quot;, &quot;label&quot;] return LightningWrapper( self.train_dataset, auto_reset=True, reader_name=&quot;ReaderX&quot;, output_map=output_map, dynamic_shape=False, ) def val_dataloader(self): output_map = [&quot;image&quot;, &quot;label&quot;] return LightningWrapper( self.valid_dataset, auto_reset=True, reader_name=&quot;ReaderX&quot;, output_map=output_map, dynamic_shape=True, ) . Here in the setup function, we use fetch_dali_loader to get the datapipeline for the train and valid stages | train_dataloader and val_dataloader is defined thank to the LightningWrapper class | . class LightningWrapper(DALIGenericIterator): def __init__(self, pipe, **kwargs): super().__init__(pipe, **kwargs) def __next__(self): out = super().__next__()[0] return out . Remark: The input of the model will be dicts of keys [“image”, “label”]. . It means . input_batch = {&quot;image&quot;: images, &quot;label&quot;: masks} . Then we also need to slight modify the train loop (training_step) and the valid loop (validation_step) of the LightningModule. . def training_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;].float(), batch[&quot;label&quot;] logits = self(imgs) train_loss = self.loss_function(logits, masks) train_dice_soft = self.dice_soft(logits, masks) self.log(&quot;train_loss&quot;, train_loss, prog_bar=True) self.log(&quot;train_dice_soft&quot;, train_dice_soft, prog_bar=True) return {&quot;loss&quot;: train_loss, &quot;train_dice_soft&quot;: train_dice_soft} . Once we finish to define LightningModule and LightningDataModule, we can jump to the Trainer to run the training. . . model = SegFormer(config.model.encoder_name, config.model.size, config.model.classes) datamodule = NailSegmentation( data_root=data_root, csv_path=csv_path, test_path=&quot;&quot;, batch_size=batch_size, num_workers=4, ) model_lighning = LitNailSegmentation(model=model, learning_rate=config.training.learning_rate) trainer = Trainer(args) trainer.fit( model=model_lighning, datamodule=datamodule ) . For more details, we can find the full source code at github . References . Segmentation Model-Part III - Training deep learning segmentation models in Pytorch Lightning | NVIDIA DALI Documentation | Augmentation Gallery | nnU-Net For PyTorch | .",
            "url": "https://hphuongdhsp.github.io/ml-blog/pytorchlightning/semanticsegmentation/deeplearning/dali/2022/08/06/segmentation-model-part5.html",
            "relUrl": "/pytorchlightning/semanticsegmentation/deeplearning/dali/2022/08/06/segmentation-model-part5.html",
            "date": " • Aug 6, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Segmentation Model-Part IV - Data augmentation on the GPU with Kornia library",
            "content": "In this post, we discover how to use Kornia modules in order to perform the data augmentation on the GPU in batch mode. Kornia is a differentiable library that allows classical computer vision to be integrated into deep learning models. Kornia consists a lot of components. One of them is kornia.augmentation - a module to perform data augmentation in the GPU. . We will work with the Segmentation Problem (Nail Segmentation). For that, we use Pytorch Lightninig to train model and use Kornia to build the data augmentation on the GPU. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . Similar to the training pipeline of the previous post, we first make the data frame to store images and masks infos. . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. . 3. The CPU bottleneck . The fact is that today these transforms are applied one input at a time on CPUs. This means that they are super slow. . 3.1 A naive approach model training . . The naive training pipeline includes: . The pre-processing of the data occurs on the CPU | The model will be typically trained on GPU/TPU. | . 3.2 Data Augmentation using GPU . To improve the training speed we can shift the data augmentation task in to GPU . . To do that we can use Kornia.augmentation, Dali libraries. . Kornia.augmentation is the module of Kornia which permit to do augmentation in GPU. It will boost the speed of traininig in almost cases. | DALI is the library for data loading and pre-processing to accelerate deep learning applications. Data processing pipelines implemented using DALI can easily be retargeted to TensorFlow, PyTorch, MXNet and PaddlePaddle. | . This post we will focus on how to use Kornia. A guide of using DALI will be introduced in next post. . 4. Data Augmentation using Kornia . In this part, we will cover how to use Kornia for data augmentation. To augumentate data on GPU, we use transforms (augumentations) as a transform_module ( in Pytorch platform, it is a nn.Module object) whose input is a tensor of size $C times H times W$ and output is also tensor of size $C times H times W$. . That transform_module is put between the processing task (includes read images, make images of batch having same size, convert images in to the tensor format) and the training model. More precisely, . class ModelWithAugumentation(nn.Module): &quot;&quot;&quot;Module to perform data augmentation on torch tensors.&quot;&quot;&quot; def __init__(self, transform_module: nn.Module, model : nn.Module) -&gt; None: super().__init__() self.transform_module = transform_module self.model = model def forward(self, x: Tensor) -&gt; Tensor: augmented_x = self.transform_module(x) # BxCxHxW x_out = self.model(augmented_x) return x_out . where transform_module is defined by using Kornia or torchvision. For example . transform_module = K.augmentation.AugmentationSequential( K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1) ) . Note: we can also use torchvsiion to define the transform module. . transforms = torch.nn.Sequential( transforms.CenterCrop(10), transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)), ) . Each transform module is a nn.Module object. We can use nn.Sequential to define a sequence of transforms. . We now apply that strategy to our problem. Comparing with the previous pipeline in the last post (Training deep learning segmentation models in Pytorch Lightning), here we have some modifications: . Only use Resize or Padding in the data augmentation on CPUs, in the last part we define the whole augmentation by using albumentations and use it as the transform before going to the model. | . import albumentations as A def resize(p: float = 1): return A.Resize(384, 384, always_apply=True) self.valid_transform = resize() self.train_transform = resize() . Using Kornia to define the augmentation (nn.Module object), here we have train_transform_K and valid_transform_K | . import kornia as K valid_transform_K = K.augmentation.AugmentationSequential( K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1), data_keys=[&quot;input&quot;, &quot;mask&quot;], ) train_transform_K = K.augmentation.AugmentationSequential( K.augmentation.container.ImageSequential( # OneOf K.augmentation.RandomHorizontalFlip(p=0.6), K.augmentation.RandomVerticalFlip(p=0.6), random_apply=1, random_apply_weights=[0.5, 0.5], ), K.augmentation.ColorJitter(0.1, 0.1, 0.1, 0.1, p=0.5), # K.augmentation.RandomAffine( degrees = (-15.0,15.0), p= 0.3), K.augmentation.Normalize(Tensor((0.485, 0.456, 0.406)), Tensor((0.229, 0.224, 0.225)), p=1), data_keys=[&quot;input&quot;, &quot;mask&quot;], same_on_batch=False, ) . In the LightningModule, we define two new functions (or two nn.Module objects) | . self.train_transform = train_transform_K self.valid_transform = valid_transform_K . Note: Add transform into the training loop and the valid loop (training_step and validation_step) . def training_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] if self.train_transform is not None: imgs, masks = self.train_transform(imgs, masks) # add the transform before going to the model imgs, masks = imgs.float(), masks.float() logits = self(imgs) train_loss = self.loss_function(logits, masks) train_dice_soft = self.dice_soft(logits, masks) self.log(&quot;train_loss&quot;, train_loss, prog_bar=True) self.log(&quot;train_dice_soft&quot;, train_dice_soft, prog_bar=True) return {&quot;loss&quot;: train_loss, &quot;train_dice_soft&quot;: train_dice_soft} def validation_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] if self.valid_transform: imgs, masks = self.valid_transform(imgs, masks) # add the transform before going to the model imgs, masks = imgs.float(), masks.float() logits = self(imgs) valid_loss = self.loss_function(logits, masks) valid_dice_soft = self.dice_soft(logits, masks) valid_iou = binary_mean_iou(logits, masks) self.log(&quot;valid_loss&quot;, valid_loss, prog_bar=True) self.log(&quot;valid_dice&quot;, valid_dice_soft, prog_bar=True) self.log(&quot;valid_iou&quot;, valid_iou, prog_bar=True) return { &quot;valid_loss&quot;: valid_loss, &quot;valid_dice&quot;: valid_dice_soft, &quot;valid_iou&quot;: valid_iou, } . We keep all of rest parts of the pipeline (LightningDataModule, Trainer). . For more details, we can find the source code at github . References . Segmentation Model-Part III - Training deep learning segmentation models in Pytorch Lightning | Kornia.augmentation | .",
            "url": "https://hphuongdhsp.github.io/ml-blog/pytorchlightning/semanticsegmentation/deeplearning/kornia/2022/08/05/segmentation-model-part4.html",
            "relUrl": "/pytorchlightning/semanticsegmentation/deeplearning/kornia/2022/08/05/segmentation-model-part4.html",
            "date": " • Aug 5, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Segmentation Model-Part III - Training deep learning segmentation models in Pytorch Lightning",
            "content": "Continue Segmentation Model the series; in this post, we discuss how to train a segmentation model in Pytorch Lightning. PyTorch Lightning is the deep learning framework for professional AI researchers and machine learning engineers who need maximal flexibility without sacrificing performance at scale. It is built on top of PyTorch. . We still work with the Segmentation Problem (Nail Segmentation) and discover some valuable tools for Pytorch Lightning. From this part, we will focus on the Pytorch Platform. Then for convenience, we recall some tasks of the previous post: Problem Description and Dataset, Data Preparation. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks. Images is the data folder, and Masks is the label folder, which is the segmentations of input images. Each folder has four sub-folder: 1, 2, 3, and 4, corresponding to four types of nail distribution. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . We want the CSV file that stores the image and mask paths. In this project, file names of images and masks are the same, and then we only need to save the images path and modify the data_root of images and masks when we define a dataset. . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. More precisely, . def make_csv_file(data_root: Union[str, Path]) -&gt; None: list_images_train_masks = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;masks&quot;)) list_images_train_images = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;images&quot;)) list_images_train = [ i for i in list_images_train_images if i in list_images_train_masks ] print(len(list_images_train)) list_images_valid = get_all_items(os.path.join(data_root, &quot;valid&quot;, &quot;masks&quot;)) train_frame = pd.DataFrame(list_images_train, columns=[&quot;images&quot;]) train_frame[&quot;train&quot;] = 1 valid_frame = pd.DataFrame(list_images_valid, columns=[&quot;images&quot;]) valid_frame[&quot;train&quot;] = 0 mkdir(f&quot;{data_root}/csv_file&quot;) train_frame.to_csv(f&quot;{data_root}/csv_file/train.csv&quot;, index=False) valid_frame.to_csv(f&quot;{data_root}/csv_file/valid.csv&quot;, index=False) . Where get_all_items, mkdir are two supported functions (defined in utils.py file) that help us to find all items in a given folder and make a new folder. . Before going define the dataloader and model, let’s recall some main features of Pytorch Lightning. For more information, you can find it at Pytorch Lightning. . 3. Pytorch Lightnining . PyTorch Lightning is an open-source, lightweight Python wrapper for machine learning researchers that is built on top of PyTorch. . With this framework, you don’t have to remember all the tiny details of the PyTorch framework because Pytorch Lightnining handles it. . Three main features of Pytorch Lightning: . LightningDataModule | LightningModule | Trainer | . 3.1 LightningDataModule . LightningDataModule is a shareable, reusable class that encapsulates all the steps needed to process data: . Data processing | Load inside Dataset | Apply transforms | Wrap inside a DataLoader | . . 3.2 LightningModule . A lightning module is composed of some components that fully define the system: . The model or system of models | The optimizer(s) | The train loop | The validation loop | . 3.3 Trainer . Once we declare LightningDataModule, LightningModule, we can train the model with Trainer API. . . A basic use of trainer: . modelmodule = LightningModule(*args_model) datamodule = LightningDataModule(*args_data) trainer = Trainer(*args_trainer) trainer.fit(modelmodule, datamodule) . 4. DataLoader . To define the LightningModule of our dataset, we first define the torch.utils.data.Dataset for the nail data. . 4.1 Define torch.utils.data.Dataset for the Nail Data . class NailDataset(Dataset): def __init__(self, data_root: str, csv_folder: str, train: str, tfms: A.Compose): self.data_root = data_root self.csv_folder = csv_folder self.train = train self.tfms = tfms if self.train == &quot;train&quot;: self.ids = pd.read_csv(os.path.join(self.csv_folder, &quot;train.csv&quot;))[&quot;images&quot;] else: self.ids = pd.read_csv(os.path.join(self.csv_folder, &quot;valid.csv&quot;))[&quot;images&quot;] def __len__(self) -&gt; int: return len(self.ids) def __getitem__(self, idx: int) -&gt; Any: fname = self.ids[idx] image = read_image(self.data_root + f&quot;/{self.train}/images&quot; + fname) mask = read_mask(self.data_root + f&quot;/{self.train}/masks&quot; + fname) mask = (mask &gt; 0).astype(np.uint8) if self.tfms is not None: augmented = self.tfms(image=image, mask=mask) image, mask = augmented[&quot;image&quot;], augmented[&quot;mask&quot;] return { &quot;image&quot;: img2tensor(image), &quot;label&quot;: img2tensor(mask), } . 4.2 Define LightningDataModule for the Nail Data . We then use LightningDataModule to wrap our NailDataset into the data module of Pytorch Lightning. . class NailSegmentation(LightningDataModule): def __init__(self, data_root: str, csv_path: str, test_path: str, batch_size: int = 16, num_workers: int = 4): super().__init__() assert os.path.isdir(csv_path), f&quot;missing folder: {csv_path}&quot; assert os.path.isdir(data_root), f&quot;missing folder: {data_root}&quot; self.data_root = str(data_root) self.csv_path = str(csv_path) self.test_path = str(test_path) self.valid_transform = valid_transform() self.train_transform = train_transform() # other configs self.batch_size = batch_size self.num_workers = num_workers if num_workers is not None else mproc.cpu_count() def prepare_data(self) -&gt; None: pass def setup(self, *_, **__) -&gt; None: self.train_dataset = NailDataset( self.data_root, self.csv_path, train=&quot;train&quot;, tfms=self.train_transform, ) self.valid_dataset = NailDataset( self.data_root, self.csv_path, train=&quot;valid&quot;, tfms=self.valid_transform, ) def train_dataloader(self) -&gt; DataLoader: return DataLoader( self.train_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True, ) def val_dataloader(self) -&gt; DataLoader: return DataLoader( self.valid_dataset, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False, ) . Here we need to define 3 main functions . set_up: data loading and preprocessing | train_dataloader(): define train_loader object in Pytorch | val_dataloader(): define val_loader object in Pytorch | . 5. Model Module . In this part we define: . A segmentation model | Wrap the model module by using LightningModule, for that we will define some main functions: def training_step : calculate {loss, metric}, logging in each train step | def validation_step: calculate {loss, metric}, logging in each valid step | def validation_epoch_end: calculate {loss, metric}, logging in each epoch by using infos of validation_step | def configure_optimizers: which optimization and learning rate scheduler do we use for the training? | . | . 5.1 Define the model by using segmentation_models_pytorch . For convenience, we use segmentation_models_pytorch to define our model. Segmentation_models_pytorch is a high-level API, it helps us build a semantic segmentation model with only some lines of code. . import segmentation_models_pytorch as smp model = smp.Unet( encoder_name=&quot;timm-efficientnet-b4&quot;, # choose encoder, e.g. mobilenet_v2 or efficientnet-b7 encoder_weights=&quot;imagenet&quot;, # use `imagenet` pre-trained weights for encoder initialization in_channels=3, # model input channels (1 for gray-scale images, 3 for RGB, classes=1, # model output channels (number of classes in your dataset) ) . Here we use: . Unet architecture for the segmentation model. Unet has two components: encoder and decoder | encoder: EfficientNet B4 which is written by timm library | input channels: 3 for RGB images, 1 for gray-scale images | classes: 1 for binary segmentation, 2 for multi-class segmentation. | . 5.2 Define LightningModule . We next use LightningModule to wrap the model into the model module of Pytorch Lightnining. . class LitNailSegmentation(LightningModule): def __init__(self, model: nn.Module, learning_rate: float = 1e-4): super().__init__() self.model = model self.loss_function = symmetric_lovasz self.dice_soft = binary_dice_coefficient self.learning_rate = learning_rate self.save_hyperparameters() def forward(self, x): return self.model(x) def training_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] imgs, masks = imgs.float(), masks.float() logits = self(imgs) train_loss = self.loss_function(logits, masks) train_dice_soft = self.dice_soft(logits, masks) self.log(&quot;train_loss&quot;, train_loss, prog_bar=True) self.log(&quot;train_dice_soft&quot;, train_dice_soft, prog_bar=True) return {&quot;loss&quot;: train_loss, &quot;train_dice_soft&quot;: train_dice_soft} def validation_step(self, batch, batch_idx): imgs, masks = batch[&quot;image&quot;], batch[&quot;label&quot;] imgs, masks = imgs.float(), masks.float() logits = self(imgs) valid_loss = self.loss_function(logits, masks) valid_dice_soft = self.dice_soft(logits, masks) valid_iou = binary_mean_iou(logits, masks) self.log(&quot;valid_loss&quot;, valid_loss, prog_bar=True) self.log(&quot;valid_dice&quot;, valid_dice_soft, prog_bar=True) self.log(&quot;valid_iou&quot;, valid_iou, prog_bar=True) return { &quot;valid_loss&quot;: valid_loss, &quot;valid_dice&quot;: valid_dice_soft, &quot;valid_iou&quot;: valid_iou, } def validation_epoch_end(self, outputs): logs = {&quot;epoch&quot;: self.trainer.current_epoch} valid_losses = torch.stack([x[&quot;valid_loss&quot;] for x in outputs]).mean() valid_dices = torch.stack([x[&quot;valid_dice&quot;] for x in outputs]).mean() valid_ious = torch.stack([x[&quot;valid_iou&quot;] for x in outputs]).mean() logs[&quot;valid_losses&quot;] = valid_losses logs[&quot;valid_dices&quot;] = valid_dices logs[&quot;valid_ious&quot;] = valid_ious return { &quot;valid_losses&quot;: valid_losses, &quot;valid_dices&quot;: valid_dices, &quot;valid_ious&quot;: valid_ious, &quot;log&quot;: logs, } def configure_optimizers(self): optimizer = torch.optim.AdamW(self.model.parameters(), lr=self.learning_rate) scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, self.trainer.max_epochs, 0) self.optimizer = [optimizer] return self.optimizer, [scheduler] . Here we use: . AdamW as the optimizers | symmetric_lovasz as the loss function, which is defined in the Loss.py file. symmetric_lovasz is defined by | . def symmetric_lovasz(outputs, targets): return 0.5*(lovasz_hinge(outputs, targets) + lovasz_hinge(-outputs, 1.0 - targets)) . where lovasz_hinge is Lovasz loss for the binary segmentation. . Metrics: Dice, IOU | . 6. Trainer . Once we have the data module, and model module, we can train the model with Trainer API, . datamodule = NailSegmentation( data_root=data_root, csv_path=csv_path, test_path=&quot;&quot;, batch_size=batch_size, num_workers=4, ) model_lighning = LitNailSegmentation(model=model, learning_rate=config.training.learning_rate) trainer = Trainer(*args_trainer) trainer.fit( model=model_lighning, datamodule=datamodule, ckpt_path=ckpt_path, ) . Here args_trainer is the argument of the trainer. More precisely, it has . { gpus: [0] # gpu device to train max_epochs: 300 # number of epochs precision: 16 # using mix precision to train auto_lr_find: True # auto find the good initial learning rate limit_train_batches: 1.0 # percent of train dataset use to train, here 100% ... } . Lightning implements various techniques to help during training that can help make the training smoother. . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/pytorchlightning/semanticsegmentation/deeplearning/2022/08/04/segmentation-model-part3.html",
            "relUrl": "/pytorchlightning/semanticsegmentation/deeplearning/2022/08/04/segmentation-model-part3.html",
            "date": " • Aug 4, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Segmentation Model-Part II - How to handle Imbalanced Data in Segmentation Problem",
            "content": "In the last post, we discussed how to train a segmentation model in Tensorflow. This post will cover how to balance datasets in training a segmentation model in Tensorflow. We can use the same technique to deal with the imbalanced data in a Classification problem. Let us recall our segmentation problem. . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the mail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks, each folder has four sub-folders 1, 2, 3, 4 corresponds to four types of distribution of nails. Images is the data folder and Masks is the label folder, which is the segmentations of input images. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . Similar to the training pipeline of the previous post, we want to have the CSV file that stores the image and mask paths . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . For that we use make_csv_file function in data_processing.py file. What thing do we need more for data balancing? . We remark that our image data have four subfolders, and the distributions of the coverage segmentation are very different in each folder. Also, the quality of the image those are different (skew data). . Folder number of image . 0 | 749 | . 1 | 144 | . 2 | 126 | . 3 | 52 | . 4 | 34 | . We want to split the info data frame into some smaller data frame. To do that we use: . def split_data_train(data_root) -&gt; None: r&quot;&quot;&quot; This function is to split the train into some subsets. The purpose of this step is to make the balanced dataset. &quot;&quot;&quot; data_root = args.data_root path_csv = f&quot;{data_root}/csv_file/train.csv&quot; train = pd.read_csv(path_csv) train[&quot;type&quot;] = train[&quot;images&quot;].apply(lambda x: x.split(&quot;/&quot;)[1]) for i in train[&quot;type&quot;].unique().tolist(): df = train.loc[train[&quot;type&quot;] == i] df.to_csv(f&quot;{data_root}/csv_file/train{i}.csv&quot;, index=False) . We have five new data frame train_0, train_1, train_2, train_3, train_4. We will use those files in the next step. . We will inherit all of the things in the previous post (DataLoader, Model, mixed precision, logger). We need to change how we load datasets and how to balance the data when we load data. . 3. How to define dataloader . We remark that all functions we will use have been defined in the last part. . For more details, we can find the source code at github . We first define all data frame and load directories of image and masks . train0_csv_dir = f&quot;{data_root}/csv_file/train0.csv&quot; train1_csv_dir = f&quot;{data_root}/csv_file/train1.csv&quot; train2_csv_dir = f&quot;{data_root}/csv_file/train2.csv&quot; train3_csv_dir = f&quot;{data_root}/csv_file/train3.csv&quot; train4_csv_dir = f&quot;{data_root}/csv_file/train4.csv&quot; train0_dataset = load_data_path(data_root, train0_csv_dir, &quot;train&quot;) train1_dataset = load_data_path(data_root, train1_csv_dir, &quot;train&quot;) train2_dataset = load_data_path(data_root, train2_csv_dir, &quot;train&quot;) train3_dataset = load_data_path(data_root, train3_csv_dir, &quot;train&quot;) train4_dataset = load_data_path(data_root, train4_csv_dir, &quot;train&quot;) . Using tf_dataset we load five datasets and remark that we will not batch in this step, we will concatenate those datasets with weights and batch them when we have the whole dataset. . The cool thing about this method is that we can use different augmentation for different sub-dataset. For example we can apply the train_transform for the first dataset and valid_transform for the second datset. . train0_loader = tf_dataset( dataset=train0_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train1_loader = tf_dataset( dataset=train1_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train2_loader = tf_dataset( dataset=train2_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train3_loader = tf_dataset( dataset=train3_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) train4_loader = tf_dataset( dataset=train4_dataset, shuffle=False, batch_size=None, transforms=train_transform(), dtype=dtype, device=args.device, ) . Shuffle and repeat each dataset . data_loaders = [ train0_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train1_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train2_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train3_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), train4_loader.apply(tf.data.experimental.shuffle_and_repeat(100000, count=epochs)), ] . Calculate the weighted sample; here we want each batch; each dataset will be loaded with the same sample. . weights = [1 / len(data_loaders)] * len(data_loaders) . Using tf.data.experimental.sample_from_datasets to balance data. . The input tf.data.experimental.sample_from_datasets function is: . datasets: A non-empty list of tf.data.Dataset objects with compatible structure. | weights: (Optional.) A list or Tensor of len(datasets) floating-point values where weights[i] represents the probability to sample from datasets[i], or a tf.data.Dataset object where each element is such a list. Defaults to a uniform distribution across datasets. | . Returns of tf.data.experimental.sample_from_datasets . A dataset that interleaves elements from datasets at random, according to weights if provided, otherwise with uniform probability. | . train_loader = tf.data.experimental.sample_from_datasets(data_loaders, weights=weights, seed=None) . We then have the train_loader with balancing data. We only need to batch them before feeding data into the model. . train_loader = train_loader.batch(batch_size) . Once we have train_loader, we define valid_loader, model, as same as the previous post. Finally, we fit the model. . history = model.fit( train_loader, steps_per_epoch=steps_per_epoch, epochs=epochs, validation_data=valid_loader, callbacks=callbacks, ) . where . steps_per_epoch = ( int( ( len(train0_dataset[0]) + len(train1_dataset[0]) + len(train2_dataset[0]) + len(train3_dataset[0]) + len(train4_dataset[0]) ) / batch_size ) + 1 ) . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/tensorflow/semanticsegmentation/deeplearning/imbalanceddata/2022/08/03/segmentation-model-part2.html",
            "relUrl": "/tensorflow/semanticsegmentation/deeplearning/imbalanceddata/2022/08/03/segmentation-model-part2.html",
            "date": " • Aug 3, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Segmentation Model-Part I - Training deep learning segmentation models in Tensorflow",
            "content": "In this post, we will cover how to train a segmentation model by using the TensorFlow platform . 1. Problem Description and Dataset . We want to cover a nail semantic segmentation problem. For each image, we want to detect the segmentation of the nail in the image. . Images Masks . | | . Our data is organized as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have two folders: Images and Masks, each folder has four sub-folders 1, 2, 3, 4 corresponds to four types of distribution of nails. Images is the data folder and Masks is the label folder, which is the segmentations of input images. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . For the convenience of loading data, we will store data information in the data frame (or CSV file). . We want to have the CSV file that stores the image and mask paths . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . To do that we use . import os from typing import Any import pandas as pd from utils import get_all_items, mkdir def make_csv_file(data_root) -&gt; None: list_images_train_masks = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;masks&quot;)) list_images_train_images = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;images&quot;)) list_images_train = [ i for i in list_images_train_images if i in list_images_train_masks ] print(len(list_images_train)) list_images_valid = get_all_items(os.path.join(data_root, &quot;valid&quot;, &quot;masks&quot;)) train_frame = pd.DataFrame(list_images_train, columns=[&quot;images&quot;]) train_frame[&quot;train&quot;] = 1 valid_frame = pd.DataFrame(list_images_valid, columns=[&quot;images&quot;]) valid_frame[&quot;train&quot;] = 0 mkdir(f&quot;{data_root}/csv_file&quot;) train_frame.to_csv(f&quot;{data_root}/csv_file/train.csv&quot;, index=False) valid_frame.to_csv(f&quot;{data_root}/csv_file/valid.csv&quot;, index=False) . Here get_all_items, mkdir are two supported functions (defined in utils.py file) that help us to find all items in a given folder and make a new folder. . Once we have the data frame, we can go to define the dataset. . 3. Define DataLoader . In this part we will do the following: . Get lists of images and masks | Define Dataloader with input being a list of images and masks and output be list of image batchs which being fed into the model. More precisely: Decode images and masks (read images and masks) | Transform data | Batch the augmented data. | . | . Before going to the next part, let’s talk about the advantages of using tf.data for the data loader pipeline. . The main feature of the next part is the data loader. We use the tensorflow.data (tf.data) to load the dataset instead of using Sequence Keras (keras.Sequence). In fact, we can also combine tf.data and keras.Sequence. This tutorial focuses on how to load data by tf.data. . Here is the pipeline loader of tf.data: . Read data from a CSV file | Transfrom (augumentate) the data | Load data into the model | . . The advantage of this method is: . Loading data by using multi-processing | Don’t have the memory leak phenomenal | Flexible to load dataset, can load weight sample data (using tf.compat.v1.data.experimental.sample_from_datasets ) | Downtime and waiting around are minimized while processing is maximized through parallel execution; see the following images: | . Naive pipeline . . This is the typical workflow of a naive data pipeline, there is always some idle time and overhead due to the inefficiency of sequential execution. . In contrast, consider: tf.data pipeline . . 3.1 Get images and masks from a dataframe. . def load_data_path(data_root: Union[str, Path], csv_dir: Union[str, Path], train: str) -&gt; Tuple: csv_file = pd.read_csv(csv_dir) ids = sorted(csv_file[&quot;images&quot;]) images = [data_root + f&quot;/{train}/images&quot; + fname for fname in ids] masks = [data_root + f&quot;/{train}/masks&quot; + fname for fname in ids] return (images, masks) . 3.2 Decode images and masks . def load_image_and_mask_from_path(image_path: tf.string, mask_path: tf.string) -&gt; Any: &quot;&quot;&quot;this function is to load image and mask Args: image_path (tf.string): the tensorflow string of image mask_path (tf.string): the tensorflow string of mask Returns: [type]: image and mask &quot;&quot;&quot; # read image by tensorflow function img = tf.io.read_file(image_path) img = tf.image.decode_image(img, channels=3) # read mask by tensorflow function mask = tf.io.read_file(mask_path) mask = tf.image.decode_image(mask, channels=1) return img, mask . 3.3 Doing augmentation . def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask . Here we use Albumentations library to define the transform. Albumentations is a Python library for fast and flexible image augmentations. Albumentations efficiently implements a rich variety of image transform operations that are optimized for performance and does so while providing a concise yet powerful image augmentation interface for different computer vision tasks, including object classification, segmentation, and detection. For example, we define our validation transform as . import albumentations as A def valid_transform(): return A.Compose( [ A.Resize(384, 384, always_apply=True), ], p=1, ) . You can find the detail of transforms in transform.py file, in the source code given at the post’s end. We remark that, after doing augmentation, we cast the output of transform into TensorFlow type tensorflow type . aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) . Once we finish the augmentation task, we can do batching of the data by . dataset = dataset.batch(batch_size) . Here, the dataset is now an object of tf.data. . Compose four previous steps, we have the data loader function: . def tf_dataset( dataset: Tuple[List[str], List[str]], shuffle: bool, batch_size: Any, transforms: A.Compose, dtype: Any, device: List[int], ): r&quot;&quot;&quot;This function is to create dataloader for tensorflow training Args: dataset Tuple[List[str], List[str]]: Tuple of List data path that have same size shuffle (bool): True if you want shuffle dataset when do training batch_size [Any]: None if you dont want spit dataset by batch transforms (A.Compose): the augumentation that you want to apple for the data Returns: datast : the prepare dataset for the training step &quot;&quot;&quot; # do augumentation by albumentations, remark that in the the end, we use tf.cast to normalize # image and mask and also make sure that the output of this function be in form of tensorflow (tf) def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask def process_data(image, mask): # using tf.numpy_function to apply the aug_img to image and mask aug_img, aug_mask = tf.numpy_function(aug_fn, [image, mask], [dtype, dtype]) return aug_img, aug_mask # convert the tuple of list (images, masks) into the tensorflow.data form dataset = tf.data.Dataset.from_tensor_slices(dataset) # apply the map reading image and mask (make sure that the input and output are in the tensorflow form (tf.)) dataset = dataset.map(load_image_and_mask_from_path, num_parallel_calls=multiprocessing.cpu_count() // len(device)) # shuffle data if shuffle: dataset = dataset.shuffle(buffer_size=100000) # do the process_data map (augumentation and normalization) dataset = dataset.map( partial(process_data), num_parallel_calls=multiprocessing.cpu_count() // len(device) ).prefetch(AUTOTUNE) # make batchsize, here we use batch_size as a parameter, in some case we dont split dataset by batchsize # for example, if we want to mix multi-dataset, then we skip this step and split dataset by batch_size later if batch_size: dataset = dataset.batch(batch_size) return dataset . 4. Define the Segmentation model . In this part, we will define the segmentation model by using segmentation_models library, we also define the loss function, optimization, and metrics. . Segmentation models is a python library with Neural Networks for Image Segmentation based on Keras (Tensorflow) framework. This is the high-level API, you need only some lines of code to create a Segmentation Neural Network. . 4.1 Model . def create_model(): model = sm.Unet( &quot;efficientnetb4&quot;, input_shape=(384, 384, 3), encoder_weights=&quot;imagenet&quot;, classes=1, ) # TO USE mixed_precision, HERE WE USE SMALL TRICK, REMOVE THE LAST LAYER AND ADD # THE ACTIVATION SIGMOID WITH THE DTYPE TF.FLOAT32 last_layer = tf.keras.layers.Activation(activation=&quot;sigmoid&quot;, dtype=tf.float32)(model.layers[-2].output) model = tf.keras.Model(model.input, last_layer) # define optimization, here we use the tensorflow addon, but use can also use some normal # optimazation that is defined in tensorflow.optimizers optimizer = tfa.optimizers.RectifiedAdam() if args.mixed_precision: optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer, dynamic=True) # define a loss fucntion dice_loss = sm.losses.DiceLoss() focal_loss = sm.losses.BinaryFocalLoss() total_loss = dice_loss + focal_loss # define metric metrics = [ sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), ] # compile model with optimizer, losses and metrics model.compile(optimizer, total_loss, metrics) return model . Here we use: . The Unet model with the backbone is efficientnetb4 | The loss function is the sum DiceLoss and FocalLoss | The metric is IOU score and FSscore | The optimization algorithm is RectifiedAdam | . 5 Model Training . Once we have: dataloader and model we then combine them to run the model. In this part we will introduce some tools that help us boost the efficiency of training: . mixed_precision | using wanbd as a callback | . 5.1 Mixed_precision . How does mixed precision work? . Mixed precision training is the use of lower-precision operations (float16 and bfloat16) in a model during training to make it run faster and use less memory. Using mixed precision can improve performance by more than 3 times on modern GPUs and 60% on TPUs. . Here is the mixed precision training flow: . . We first feed the data as the float16 or bloat16 type, then the input of the model has the low type (float16 and bfloat16). | All of the calculations in the model are computed with the lower-precision operations | Convert the output of the model into float32 to do optimization tasks. | Update weights, convert them into lower-precision, and continue the next round of training. | . To train the model in TensorFlow with mixed precision, we just modify: . We first define the global policy: | . if args.mixed_precision: policy = mixed_precision.Policy(&quot;mixed_float16&quot;) mixed_precision.set_policy(policy) print(&quot;Mixed precision enabled&quot;) . Change the out data (input of model) into tf.float16: | . When we load dataset, before do suffling and do batching we convert out data into float16. To do that, . def process_data(image, mask): # using tf.numpy_function to apply the aug_img to image and mask aug_img, aug_mask = tf.numpy_function(aug_fn, [image, mask], [dtype, dtype]) return aug_img, aug_mask . Fix the last layer of the model. Here we remark that the dtype of the last layer should be float32. To do that, in the model part, we add some trick lines: | . model = sm.Unet( &quot;efficientnetb4&quot;, input_shape=(384, 384, 3), encoder_weights=&quot;imagenet&quot;, classes=1, ) # TO USE mixed_precision, HERE WE USE SMALL TRICK, REMOVE THE LAST LAYER AND ADD # THE ACTIVATION SIGMOID WITH THE DTYPE TF.FLOAT32 last_layer = tf.keras.layers.Activation(activation=&quot;sigmoid&quot;, dtype=tf.float32)( model.layers[-2].output ) . 5.2 Using Wanbd for logging. . In this part, we will cover how to use wandb for logging. WandB is a central dashboard to keep track of your hyperparameters, system metrics, and predictions so you can compare models live and share your findings. To do that we use callback of model training as the WandbLogging . import wandb from wandb.keras import WandbCallback logdir = f&quot;{work_dir}/tensorflow/logs/wandb&quot; mkdir(logdir) wandb.init(project=&quot;Segmentation by Tensorflow&quot;, dir=logdir) wandb.config = { &quot;learning_rate&quot;: earning_rate, &quot;epochs&quot;: epochs, &quot;batch_size&quot;: batch_size, } callbacks.append(WandbCallback()) . We finish the training task by calling the train loader and the valid loader and fitting the model. Then . 5.3 Dataloader . data_root = str(args.data_root) train_csv_dir = f&quot;{data_root}/csv_file/train.csv&quot; valid_csv_dir = f&quot;{data_root}/csv_file/valid.csv&quot; # set batch_size batch_size = args.batch_size epochs = args.epochs # get training and validation set train_dataset = load_data_path(data_root, train_csv_dir, &quot;train&quot;) train_loader = tf_dataset( dataset=train_dataset, shuffle=True, batch_size=batch_size, transforms=train_transform(), dtype=dtype, device=args.device, ) valid_dataset = load_data_path(data_root, valid_csv_dir, &quot;valid&quot;) valid_loader = tf_dataset( dataset=valid_dataset, shuffle=False, batch_size=batch_size, transforms=valid_transform(), dtype=dtype, device=args.device, ) . 5.4 Fit training . history = model.fit( train_loader, steps_per_epoch=total_steps, epochs=epochs, validation_data=valid_loader, callbacks=callbacks, ) . For more details, we can find the source code at github .",
            "url": "https://hphuongdhsp.github.io/ml-blog/tensorflow/semanticsegmentation/deeplearning/2022/08/02/segmentation-model-part1.html",
            "relUrl": "/tensorflow/semanticsegmentation/deeplearning/2022/08/02/segmentation-model-part1.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hphuongdhsp.github.io/ml-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I’am Hoang-Phuong. . I am currently working at MedTech Torus-Actions as a Data Scientist. . My passion is in several domains: Machine Learning, Cryptocurrency, and Fintech. I like to follow the latest technology in Deep Learning. . Besides work, I like to play sports: football, billard, ping-pong,… .",
          "url": "https://hphuongdhsp.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  
      ,"page2": {
          "title": "",
          "content": "SHARE2LEARN Machine Learning Blog . . **Welcome! My latest articles are below, if you’d like to get in touch, find me at @hphuongdhsp . Posts 👇 .",
          "url": "https://hphuongdhsp.github.io/ml-blog/",
          "relUrl": "/",
          "date": ""
      }
      
  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hphuongdhsp.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}