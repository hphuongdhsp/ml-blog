<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Segmentation Model-Part VII - Training Instance Segmentation in MMDetection | Hoang Phuong’ Blog</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Segmentation Model-Part VII - Training Instance Segmentation in MMDetection" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="The seventh part of the Segmentation Tutorial Series, a step-by-step guide to developing Instance Segmentation Models in MMDetection" />
<meta property="og:description" content="The seventh part of the Segmentation Tutorial Series, a step-by-step guide to developing Instance Segmentation Models in MMDetection" />
<link rel="canonical" href="https://hphuongdhsp.github.io/ml-blog/2022/08/06/segmentation-model-part7.html" />
<meta property="og:url" content="https://hphuongdhsp.github.io/ml-blog/2022/08/06/segmentation-model-part7.html" />
<meta property="og:site_name" content="Hoang Phuong’ Blog" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-08-06T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Segmentation Model-Part VII - Training Instance Segmentation in MMDetection" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-08-06T00:00:00-05:00","datePublished":"2022-08-06T00:00:00-05:00","description":"The seventh part of the Segmentation Tutorial Series, a step-by-step guide to developing Instance Segmentation Models in MMDetection","headline":"Segmentation Model-Part VII - Training Instance Segmentation in MMDetection","mainEntityOfPage":{"@type":"WebPage","@id":"https://hphuongdhsp.github.io/ml-blog/2022/08/06/segmentation-model-part7.html"},"url":"https://hphuongdhsp.github.io/ml-blog/2022/08/06/segmentation-model-part7.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/ml-blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hphuongdhsp.github.io/ml-blog/feed.xml" title="Hoang Phuong' Blog" /><link rel="shortcut icon" type="image/x-icon" href="/ml-blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/ml-blog/">Hoang Phuong&#39; Blog</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/ml-blog/about/">About Me</a><a class="page-link" href="/ml-blog/search/">Search</a><a class="page-link" href="/ml-blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Segmentation Model-Part VII -  Training Instance Segmentation in MMDetection</h1><p class="page-description">The seventh part of the Segmentation Tutorial Series, a step-by-step guide to developing Instance Segmentation Models in MMDetection</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-08-06T00:00:00-05:00" itemprop="datePublished">
        Aug 6, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h2"><a href="#1-semantic-segmentation-vs-instance-segmentation">1. Semantic Segmentation vs Instance Segmentation</a></li>
<li class="toc-entry toc-h2"><a href="#1-problem-description-and-dataset">1. Problem Description and Dataset</a>
<ul>
<li class="toc-entry toc-h3"><a href="#mission-we-want-to-have-a-bounding-box-and-segmentation-of-each-nail-in-the-picture">Mission: We want to have a bounding box and segmentation of each nail in the picture.</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#2-data-preparation">2. Data Preparation</a>
<ul>
<li class="toc-entry toc-h3"><a href="#21-make-data-frame">2.1 Make data frame</a></li>
<li class="toc-entry toc-h3"><a href="#22-get-coco-annotation">2.2 Get coco annotation</a></li>
</ul>
</li>
<li class="toc-entry toc-h2"><a href="#3-training-instance-segmentation-problems-by-mmdetection">3. Training instance segmentation problems by MMDetection</a>
<ul>
<li class="toc-entry toc-h3"><a href="#31-mmdetection">3.1 MMDetection</a></li>
<li class="toc-entry toc-h3"><a href="#32-modify-the-config">3.2 Modify the config.</a>
<ul>
<li class="toc-entry toc-h4"><a href="#modify-the-model-config">Modify the model config</a></li>
<li class="toc-entry toc-h4"><a href="#modify-the-data-pipeline-config">Modify the data pipeline config</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#32-training">3.2 Training</a></li>
<li class="toc-entry toc-h3"><a href="#references">References</a></li>
</ul>
</li>
</ul><p>In this post, we will cover how to train a instance segmentation model by using the MMDetection library.</p>

<h2 id="1-semantic-segmentation-vs-instance-segmentation">
<a class="anchor" href="#1-semantic-segmentation-vs-instance-segmentation" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Semantic Segmentation vs Instance Segmentation</h2>

<p>We first introduce about: Semantic image segmentation, Object detection, Semantic Image segmentation</p>

<ul>
  <li>Semantic image segmentation marks all pixels belonging to that tag, but won’t define the boundaries of each object.</li>
  <li>Object detection does not segment the object, but define the location of each individual object instance with a box.</li>
  <li>Combining semantic segmentation with object detection leads to instance segmentation</li>
</ul>

<p><img align="center" width="600" src="https://habrastorage.org/webt/uc/uw/sy/ucuwsy-0vb8vjqw0v_9gviyv-ga.jpeg"></p>

<p>Source: V7Lab</p>

<p>Nowaday, to tackle the instance segmentation problem, one use uselly <a href="https://arxiv.org/pdf/1703.06870.pdf">Mask R-CNN model</a> which is presented by [K.He] and all.  For more detail about Mask R-CNN model, we  refer to read <a href="https://viso.ai/deep-learning/mask-r-cnn/#:~:text=Mask%20R%2DCNN%20is%20a,segmentation%20mask%20for%20each%20instance.">Everything about Mask R-CNN: A Beginner’s Guide</a> artical.</p>

<p>Mask R-CNN is the state-of-the-art model for Instance Segmentation with three outputs: mask, classes and boundary box.</p>

<p><img align="center" width="600" src="https://habrastorage.org/webt/kg/sg/eb/kgsgebllp-5ajlord4ikausbzle.png"></p>

<p>Source: V7Lab</p>

<h2 id="1-problem-description-and-dataset">
<a class="anchor" href="#1-problem-description-and-dataset" aria-hidden="true"><span class="octicon octicon-link"></span></a>1. Problem Description and Dataset</h2>

<p>We will cover the nail instance segmentation. We want to have a bounding box and segment each nail in the picture. It’s from the real application. For example, we want to make a nail disease classification application. To do that, we first do the nail segmentation task. Then we can crop nail parts and drop the cropped nail into the nail classification model in order to classify disease for each nail.</p>

<p>For the semantic nail segmentation, we can segment the nail part and then use post-processing to obtain the cropped nail images. That strategy sometimes does not work when the nail positions overlap. We then want to try instance segmentation to tackle that difficulty.</p>

<table>
  <thead>
    <tr>
      <th style="text-align: center">Images</th>
      <th style="text-align: center">Masks</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: center"><img align="center" width="300" src="https://habrastorage.org/webt/em/og/9v/emog9v4ya7ssllg5dht77_wehqk.png"></td>
      <td style="text-align: center"><img align="center" width="300" src="https://habrastorage.org/webt/hl/bf/ov/hlbfovx1uhrbbebgxndyho9yywo.png"></td>
    </tr>
  </tbody>
</table>

<h3 id="mission-we-want-to-have-a-bounding-box-and-segmentation-of-each-nail-in-the-picture">
<a class="anchor" href="#mission-we-want-to-have-a-bounding-box-and-segmentation-of-each-nail-in-the-picture" aria-hidden="true"><span class="octicon octicon-link"></span></a>Mission: <strong>We want to have a bounding box and segmentation of each nail in the picture.</strong>
</h3>

<p>Our data is organized as</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── Images
│   ├── 1
│       ├── first_image.png
│       ├── second_image.png
│       ├── third_image.png
│   ├── 2
│   ├── 3
│   ├── 4
├── Masks
│   ├── 1
│       ├── first_image.png
│       ├── second_image.png
│       ├── third_image.png
│   ├── 2
│   ├── 3
│   ├── 4

</code></pre></div></div>

<p>We have two folders: <code class="language-plaintext highlighter-rouge">Images</code> and <code class="language-plaintext highlighter-rouge">Masks</code>. <code class="language-plaintext highlighter-rouge">Images</code> is the data folder, and <code class="language-plaintext highlighter-rouge">Masks</code> is the label folder, which is the segmentations of input images. Each folder has four sub-folder:  <code class="language-plaintext highlighter-rouge">1</code>, <code class="language-plaintext highlighter-rouge">2</code>, <code class="language-plaintext highlighter-rouge">3</code>, and <code class="language-plaintext highlighter-rouge">4</code>, corresponding to four types of nail distribution.</p>

<p>We download data from <a href="https://drive.google.com/file/d/1qBLwdQeu9nvTw70E46XNXMciB0aKsM7r/view?usp=sharing">link</a> and put it in <code class="language-plaintext highlighter-rouge">data_root</code>, for example</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data_root</span> <span class="o">=</span> <span class="s">"./nail-segmentation-dataset"</span>
</code></pre></div></div>

<h2 id="2-data-preparation">
<a class="anchor" href="#2-data-preparation" aria-hidden="true"><span class="octicon octicon-link"></span></a>2. Data Preparation</h2>

<p>We now have only the semantic segmentation dataset. This part we will make the instance segmentation datset and save that data in the form <code class="language-plaintext highlighter-rouge">coco</code>.</p>

<h3 id="21-make-data-frame">
<a class="anchor" href="#21-make-data-frame" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.1 Make data frame</h3>

<p>For convenient, we will save all of dataset information in the csv files:</p>

<p>images,masks,width,height</p>

<table>
  <thead>
    <tr>
      <th>images</th>
      <th>masks</th>
      <th>width</th>
      <th>height</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>images/1/filename1.png</td>
      <td>masks/1/filename1.png</td>
      <td>256</td>
      <td>256</td>
    </tr>
    <tr>
      <td>images/1/filename1.png</td>
      <td>masks/1/filename1.png</td>
      <td>256</td>
      <td>256</td>
    </tr>
    <tr>
      <td>images/2/filename1.png</td>
      <td>masks/2/filename1.png</td>
      <td>256</td>
      <td>256</td>
    </tr>
    <tr>
      <td>images/2/filename1.png</td>
      <td>masks/2/filename1.png</td>
      <td>256</td>
      <td>256</td>
    </tr>
  </tbody>
</table>

<p>The function <a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/blob/master/Part%207-Instance%20Segmentation%20with%20MMDetection/data_preprocessing.py"><code class="language-plaintext highlighter-rouge">make_csv_file</code></a> helps us do the above task.</p>

<p>To do that we use two functions <code class="language-plaintext highlighter-rouge">png2numpy</code>, <code class="language-plaintext highlighter-rouge">make_csv_file_npy</code> in <a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/blob/master/Part%205-Pytorch%20with%20Dali/data_processing.py"><code class="language-plaintext highlighter-rouge">data_processing.py</code></a> file.</p>

<h3 id="22-get-coco-annotation">
<a class="anchor" href="#22-get-coco-annotation" aria-hidden="true"><span class="octicon octicon-link"></span></a>2.2 Get coco annotation</h3>

<p>We want to convert our semantic segmentation data into the instance segmentaion. One of the famous format to organize the instance segmentation data is <code class="language-plaintext highlighter-rouge">COCO</code>.</p>

<p>The coco annotation has the following format</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>{
    "images": [images],
    "annotations": [annotations],
    "categories": [categories]
}
</code></pre></div></div>

<p>Where:</p>

<ul>
  <li>
<code class="language-plaintext highlighter-rouge">"images"</code> (type: [List[Dict]]) is the list of dictionaries, each dictionary has informations
    <ul>
      <li>“id”: 100   The id of image</li>
      <li>“file_name”: “train/images/1/image_100.png”,  the path to get image</li>
      <li>“width”: 1800,</li>
      <li>“height”: 1626</li>
    </ul>
  </li>
  <li>
    <p><code class="language-plaintext highlighter-rouge">"annotations"</code>  is the list of dictionaries, each dictionary has informations</p>

    <ul>
      <li>“id”: 350, id of object (not the image id)</li>
      <li>“image_id”: 100, id of image</li>
      <li>“category_id”: 1, id of categories</li>
      <li>“segmentation”: RLE or [polygon],</li>
      <li>“area”: float,</li>
      <li>“bbox”: [x,y,width,height],</li>
      <li>“iscrowd”: 0 or 1,</li>
    </ul>
  </li>
  <li>
<code class="language-plaintext highlighter-rouge">"categories"</code>  is the list of dictionaries, each dictionary has informations
    <ul>
      <li>“id”: int = 0 id of categories</li>
      <li>“name”: str = “nail”</li>
    </ul>
  </li>
</ul>

<p>Using the get_annotations function, we can convert the semantic segmentation data into the coco format data of the instance segmentation.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_annotations(dataframe: pd.DataFrame):
    """get_annotations is to convert a dataframe into the coco format

    Args:
        train_df (pd.DataFrame): the dataframe that stored the infomation
        of the dataset. the form of the dataframe is
        images | width | height |

    Returns:
        [type]: the coco format data of the dataset
    """

    cats = [{"id": 0, "name": "nail"}]

    annotations = []
    images = []
    obj_count = 0

    for idx, row in tqdm(dataframe.iterrows(), total=len(dataframe)):
        filename = row.images

        images.append(
            {
                "id": idx,
                "file_name": filename,
                "width": row.width,
                "height": row.height,
            }
        )

        binary_mask = read_mask(os.path.join(str(data_root), row.masks))

        contours = find_contours(binary_mask)

        for contour in contours:
            xmin = int(np.min(contour[:, :, 0]))
            xmax = int(np.max(contour[:, :, 0]))
            ymin = int(np.min(contour[:, :, 1]))
            ymax = int(np.max(contour[:, :, 1]))

            poly = contour.flatten().tolist()
            poly = [x + 0.5 for x in poly]

            data_anno = {
                "image_id": idx,
                "id": obj_count,
                "category_id": 0,
                "bbox": [xmin, ymin, (xmax - xmin), (ymax - ymin)],
                "area": (xmax - xmin) * (ymax - ymin),
                "segmentation": [poly],
                "iscrowd": 0,
            }
            if (xmax - xmin) * (ymax - ymin) &lt; 20:
                continue

            else:
                annotations.append(data_anno)

                obj_count += 1

    return {"categories": cats, "images": images, "annotations": annotations}

</code></pre></div></div>

<p>Where:</p>

<ul>
  <li>find_contours is a function to get contour of a binary mask.</li>
  <li>dataframe is the data frame obtained from the make_csv_file function, that have the infomations of data.</li>
</ul>

<p>We then save the annotaions as a json file by the <code class="language-plaintext highlighter-rouge">get_json_coco</code> function</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def get_json_coco(args) -&gt; None:
    train_df = pd.read_csv(f"{data_root}/csv_file/train_info.csv")
    valid_df = pd.read_csv(f"{data_root}/csv_file/valid_info.csv")

    coco_json = os.path.join(data_root, "annotations")
    mkdir(coco_json)
    train_json = get_annotations(train_df)
    valid_json = get_annotations(valid_df)

    with open(f"{coco_json}/train.json", "w+", encoding="utf-8") as f:
        json.dump(train_json, f, ensure_ascii=True, indent=4)
    with open(f"{coco_json}/valid.json", "w+", encoding="utf-8") as f:
        json.dump(valid_json, f, ensure_ascii=True, indent=4)

</code></pre></div></div>

<p><strong>For more details, we can find the source code at <a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%205-Pytorch%20with%20Dali">github</a></strong></p>

<h2 id="3-training-instance-segmentation-problems-by-mmdetection">
<a class="anchor" href="#3-training-instance-segmentation-problems-by-mmdetection" aria-hidden="true"><span class="octicon octicon-link"></span></a>3. Training instance segmentation problems by MMDetection</h2>

<h3 id="31-mmdetection">
<a class="anchor" href="#31-mmdetection" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.1 MMDetection</h3>

<p>MMDetection is an object detection toolbox that contains a rich set of object detection and instance segmentation methods as well as related components and modules. It is built on top of PyTorch.</p>

<p>One decomposes the detection framework into different components and one can easily construct a customized object detection framework by combining different modules. In this part, we discover how to decompose  the instance segmentation framework and modify them in order to train a instance segmentation model.</p>

<p>To train a instance segmentation or object detection model, we pass to three steps:</p>

<ul>
  <li>Prepare the customized dataset</li>
  <li>Prepare a config</li>
  <li>Train, test, inference models on the customized dataset.</li>
</ul>

<p>In the second part we have customized our dataset into the coco format. With the coco format, we can easy reuse configurations.</p>

<h3 id="32-modify-the-config">
<a class="anchor" href="#32-modify-the-config" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2 Modify the config.</h3>

<p><strong>Config is all we need</strong>.</p>

<p>To run a instance segmentation or object detection, all we need to do is define a good config. In the config file, there are all of infomation for a training model.</p>

<p>Examples of configurations are given in <a href="https://github.com/open-mmlab/mmdetection/tree/master/configs">config</a>. There are a lot of configs that help to build a customized configs. For the convenience, we will download them and put them to the repository of <a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%207-Instance%20Segmentation%20with%20MMDetection">MMDetection tutorial</a>.</p>

<p>A Config can be decompose into four parts.</p>

<ul>
  <li>model: define the model architechture, loss function</li>
  <li>dataset: define the data pipeline</li>
  <li>schedules: define the optimization and the schedules learning rate</li>
  <li>default_runtime: define the logging, check point.</li>
</ul>

<p>In the <code class="language-plaintext highlighter-rouge">configs/__base__</code> there are examples for each module</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>├── configs
│   ├── __base__
│       ├── datasets
│       ├── models
│       ├── schedules
│       ├── default_runtime.py
</code></pre></div></div>

<p>Also, inside of the <code class="language-plaintext highlighter-rouge">configs</code>, we have alot of subconfigs that coresponding to the model acrchitecture.</p>

<p>For example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>configs/mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py

</code></pre></div></div>

<p>Here we can see:</p>

<ul>
  <li>mask_rcnn: type of mask_rcnn</li>
  <li>r50: backbone of the model (Resnet50)</li>
  <li>caffe: the pretrained model is caffe model.</li>
  <li>fpn: the feature pyramid network.</li>
  <li>mstrain: the multi-scale image for the data pipeline</li>
  <li>poly: schedule poly</li>
  <li>1x: 12 max_epochs</li>
  <li>coco: the dataset is coco format.</li>
</ul>

<p>In this post, we focus on two module: dataset and model, we set the schedules and default_runtime as default.</p>

<h4 id="modify-the-model-config">
<a class="anchor" href="#modify-the-model-config" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modify the model config</h4>
<p>For the nail segmentation, the output is a binary mask (only one type of object), we then redefine the model as:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># The new config inherits a base config to highlight the necessary modification
_base_ = "mask_rcnn/mask_rcnn_r50_caffe_fpn_mstrain-poly_1x_coco.py"

# We also need to change the num_classes in head to match the dataset's annotation
model = dict(roi_head=dict(bbox_head=dict(num_classes=1), mask_head=dict(num_classes=1)))

</code></pre></div></div>

<h4 id="modify-the-data-pipeline-config">
<a class="anchor" href="#modify-the-data-pipeline-config" aria-hidden="true"><span class="octicon octicon-link"></span></a>Modify the data pipeline config</h4>

<p>For the data pipeline we use:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>data = dict(
    samples_per_gpu=2,
    workers_per_gpu=2,
    train=dict(
        type="CocoDataset",
        img_prefix=data_root,
        classes=cfg.classes,
        ann_file=f"{data_root}/annotations/train.json",
        pipeline=cfg.train_pipeline,
    ),
    val=dict(
        type="CocoDataset",
        img_prefix=data_root,
        classes=cfg.classes,
        ann_file=f"{data_root}/annotations/valid.json",
        pipeline=cfg.test_pipeline,
    ),
    test=dict(
        type="CocoDataset",
        img_prefix=data_root,
        classes=cfg.classes,
        ann_file=f"{data_root}/annotations/valid.json",
        pipeline=cfg.test_pipeline,
    ),
)

</code></pre></div></div>

<p>Here:</p>
<ul>
  <li>img_prefix: - the path to the image directory.</li>
  <li>ann_file: the path to the json annotation file.</li>
  <li>classes: the classes of the dataset. Here class: = [“nail”]</li>
  <li>pipeline: data pipeline processing that is defined as</li>
</ul>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_pipeline = [
    dict(type="LoadImageFromFile"),
    dict(type="LoadAnnotations", with_bbox=True, with_mask=True),
    dict(
        type="Resize",
        img_scale=[(1333, 640), (1333, 672), (1333, 704), (1333, 736), (1333, 768), (1333, 800)],
        multiscale_mode="value",
        keep_ratio=True,
    ),
    dict(type="RandomFlip", flip_ratio=0.5),
    dict(type="Normalize", **img_norm_cfg),
    dict(type="Pad", size_divisor=32),
    dict(type="DefaultFormatBundle"),
    dict(type="Collect", keys=["img", "gt_bboxes", "gt_labels", "gt_masks"]),
]
test_pipeline = [
    dict(type="LoadImageFromFile"),
    dict(
        type="MultiScaleFlipAug",
        img_scale=(1333, 800),
        flip=False,
        transforms=[
            dict(type="Resize", keep_ratio=True),
            dict(type="RandomFlip"),
            dict(type="Normalize", **img_norm_cfg),
            dict(type="Pad", size_divisor=32),
            dict(type="ImageToTensor", keys=["img"]),
            dict(type="Collect", keys=["img"]),
        ],
    ),
]
</code></pre></div></div>

<h3 id="32-training">
<a class="anchor" href="#32-training" aria-hidden="true"><span class="octicon octicon-link"></span></a>3.2 Training</h3>

<p>Once we have the config file (see [nail_conig.py], we continue to train model.</p>

<p>We first call the config by using the <code class="language-plaintext highlighter-rouge">Config</code> of <code class="language-plaintext highlighter-rouge">mmcv</code> library:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>cfg = mmcv.Config("configs/nail_config.py")
</code></pre></div></div>

<p>Using the apis: <code class="language-plaintext highlighter-rouge">build_detector</code>, <code class="language-plaintext highlighter-rouge">build_dataset</code> of mmdetection library, we can easily build the model and dataset.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from mmdet.apis import init_random_seed, set_random_seed, train_detector

model = build_detector(cfg.model, train_cfg=cfg.get("train_cfg"), test_cfg=cfg.get("test_cfg"))
model.init_weights()

datasets = [build_dataset(cfg.data.train)]
</code></pre></div></div>

<p>Finally we use the <code class="language-plaintext highlighter-rouge">train_detector</code> as the Training API:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>train_detector(model, datasets, cfg)
</code></pre></div></div>

<p>After 40 epochs, we can see the model is training well.</p>

<p><img align="center" width="400" src="https://habrastorage.org/webt/q1/5v/hw/q15vhwbx5bslvd97sfhpca1iffu.jpeg"></p>

<p><strong>For more details, we can find the source code at <a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%207-Instance%20Segmentation%20with%20MMDetection">github</a></strong></p>

<h3 id="references">
<a class="anchor" href="#references" aria-hidden="true"><span class="octicon octicon-link"></span></a>References</h3>

<ul>
  <li><a href="https://github.com/hphuongdhsp/Segmentation-Tutorial/tree/master/Part%207-Instance%20Segmentation%20with%20MMDetection">Part 7-Instance Segmentation with MMDetection</a></li>
  <li><a href="https://mmdetection.readthedocs.io/en/stable/2_new_data_model.html#train-with-customized-datasets">MMDetection Tutorial - TRAIN WITH CUSTOMIZED DATASETS</a></li>
</ul>

  </div><a class="u-url" href="/ml-blog/2022/08/06/segmentation-model-part7.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/ml-blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/ml-blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/ml-blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p></p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/hphuongdhsp" target="_blank" title="hphuongdhsp"><svg class="svg-icon grey"><use xlink:href="/ml-blog/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/hphuongdhsp" target="_blank" title="hphuongdhsp"><svg class="svg-icon grey"><use xlink:href="/ml-blog/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
