{
  
    
        "post0": {
            "title": "Segmentation Model-Part I - Training deep learning models using Tensorflow platform",
            "content": "In this part we will cover how to train a segmentation model by using the tensorflow platform . Outline . 1.Problem Description and Dataset | 2. Data Preparation | 3. Define Dataloader 3.1. Decode images | 3.2. Process Data | 3.3. Batching Data | . | . 1. Problem Description and Dataset . We will cover the nail semantic segmentation. For each image we want to detect the segmentation of the mail in the image. . Images Masks . | | . Our original data is organizated as . ├── Images │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 ├── Masks │ ├── 1 │ ├── first_image.png │ ├── second_image.png │ ├── third_image.png │ ├── 2 │ ├── 3 │ ├── 4 . We have 2 folders: Images and Masks, each folder has four sub-folders 1, 2, 3, 4 corresponds to four types of distribution of nail. Images is the input folder and Masks is the label folder, that is the segmentations that we want to detect. . We download data from link and put it in data_root, for example . data_root = &quot;./nail-segmentation-dataset&quot; . 2. Data Preparation . For convenience of loading data, we will store information of data in the dataframe (or csv file). . We want to have the a csv file that store the images and masks path . index images . 1 | path_first_image.png | . 2 | path_second_image.png | . 3 | path_third_image.png | . 4 | path_fourth_image.png | . To do that we use . import os from typing import Any import pandas as pd from utils import get_all_items, mkdir def make_csv_file(data_root) -&gt; None: list_images_train_masks = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;masks&quot;)) list_images_train_images = get_all_items(os.path.join(data_root, &quot;train&quot;, &quot;images&quot;)) list_images_train = [ i for i in list_images_train_images if i in list_images_train_masks ] print(len(list_images_train)) list_images_valid = get_all_items(os.path.join(data_root, &quot;valid&quot;, &quot;masks&quot;)) train_frame = pd.DataFrame(list_images_train, columns=[&quot;images&quot;]) train_frame[&quot;train&quot;] = 1 valid_frame = pd.DataFrame(list_images_valid, columns=[&quot;images&quot;]) valid_frame[&quot;train&quot;] = 0 mkdir(f&quot;{data_root}/csv_file&quot;) train_frame.to_csv(f&quot;{data_root}/csv_file/train.csv&quot;, index=False) valid_frame.to_csv(f&quot;{data_root}/csv_file/valid.csv&quot;, index=False) . Here get_all_items, mkdir are two supported functions, that help us to find all items in a given folder and make new folder. . Once we have csv, we can pass to define the dataset. . 3. Define DataLoader . In this part we will do the following: . Get lists of images and masks | Define Dataloader with input is lists of images and masks and outout is list of batch image which is feed into the model. More precisely, in this step we will: Decode images and masks | Doing Augumentation | Batching the augumented data. | . | . 3.0 Get lists of images and masks . def load_data_path(data_root: Union[str, Path], csv_dir: Union[str, Path], train: str) -&gt; Tuple: csv_file = pd.read_csv(csv_dir) ids = sorted(csv_file[&quot;images&quot;]) images = [data_root + f&quot;/{train}/images&quot; + fname for fname in ids] masks = [data_root + f&quot;/{train}/masks&quot; + fname for fname in ids] return (images, masks) . 3.1 Decode images and masks . def load_image_and_mask_from_path(image_path: tf.string, mask_path: tf.string) -&gt; Any: &quot;&quot;&quot;this function is to load image and mask Args: image_path (tf.string): the tensorflow string of image mask_path (tf.string): the tensorflow string of mask Returns: [type]: image and mask &quot;&quot;&quot; # read image by tensorflow function img = tf.io.read_file(image_path) img = tf.image.decode_image(img, channels=3) # read mask by tensorflow function mask = tf.io.read_file(mask_path) mask = tf.image.decode_image(mask, channels=1) return img, mask . 3.2 Doing augumentaion . def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask . Here we use albumentation library to define the transform, for example . import albumentations as A def valid_transform(): return A.Compose( [ A.Resize(384, 384, always_apply=True), ], p=1, ) . We remark that, after doing augumentation, we cast the output of transfrom into tensorflow type . aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) . Once we finish the augumentation task, we can do batch of the data by . dataset = dataset.batch(batch_size) . Compose 4 privious step we have: . def tf_dataset( dataset: Tuple[List[str], List[str]], shuffle: bool, batch_size: Any, transforms: A.Compose, dtype: Any, device: List[int], ): r&quot;&quot;&quot;This function is to create dataloader for tensorflow training Args: dataset Tuple[List[str], List[str]]: Tuple of List data path that have same size shuffle (bool): True if you want shuffle dataset when do training batch_size [Any]: None if you dont want spit dataset by batch transforms (A.Compose): the augumentation that you want to apple for the data Returns: datast : the prepare dataset for the training step &quot;&quot;&quot; # do augumentation by albumentations, remark that in the the end, we use tf.cast to normalize # image and mask and also make sure that the out put of this function be in form of tensorflow (tf) def aug_fn(image, mask): # do augumentation by albumentations library data = {&quot;image&quot;: image, &quot;mask&quot;: mask} aug_data = transforms(**data) aug_img = aug_data[&quot;image&quot;] aug_mask = aug_data[&quot;mask&quot;] # do normalize by using the tensorflow.cast function aug_img = tf.cast(aug_img / 255.0, dtype) aug_mask = tf.cast(aug_mask / 255.0, dtype) return aug_img, aug_mask def process_data(image, mask): # using tf.numpy_function to apply the aug_img to image and mask aug_img, aug_mask = tf.numpy_function(aug_fn, [image, mask], [dtype, dtype]) return aug_img, aug_mask # convert the tuple of list (images, masks) into the tensorflow.data form dataset = tf.data.Dataset.from_tensor_slices(dataset) # apply the map reading image and mask (make sure that the input and output are in the tensorflow form (tf.)) dataset = dataset.map(load_image_and_mask_from_path, num_parallel_calls=multiprocessing.cpu_count() // len(device)) # shuffle data if shuffle: dataset = dataset.shuffle(buffer_size=100000) # do the process_data map (augumentation and normalization) dataset = dataset.map( partial(process_data), num_parallel_calls=multiprocessing.cpu_count() // len(device) ).prefetch(AUTOTUNE) # make batchsize, here we use batch_size as a parameter, in some case we dont split dataset by batchsize # for example, if we want to mix multi-dataset, then we skip this step and split dataset by batch_size later if batch_size: dataset = dataset.batch(batch_size) return dataset . 4. Define the Segmentation model . This part we will define the segmentation model by using segmentation_models library, we also define the loss function, optimization and metric . 4.1 Model . def create_model(): model = sm.Unet( &quot;efficientnetb4&quot;, input_shape=(384, 384, 3), encoder_weights=&quot;imagenet&quot;, classes=1, ) # TO USE mixed_precision, HERE WE USE SMALL TRICK, REMOVE THE LAST LAYER AND ADD # THE ACTIVATION SIGMOID WITH THE DTYPE TF.FLOAT32 last_layer = tf.keras.layers.Activation(activation=&quot;sigmoid&quot;, dtype=tf.float32)(model.layers[-2].output) model = tf.keras.Model(model.input, last_layer) # define optimization, here we use the tensorflow addon, but use can also use some normal # optimazation that is defined in tensorflow.optimizers optimizer = tfa.optimizers.RectifiedAdam() if args.mixed_precision: optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer, dynamic=True) # define a loss fucntion dice_loss = sm.losses.DiceLoss() focal_loss = sm.losses.BinaryFocalLoss() total_loss = dice_loss + focal_loss # define metric metrics = [ sm.metrics.IOUScore(threshold=0.5), sm.metrics.FScore(threshold=0.5), ] # compile model with optimizer, losses and metrics model.compile(optimizer, total_loss, metrics) return model .",
            "url": "https://hphuongdhsp.github.io/ml-blog/markdown/2022/08/02/segmentation-model-part1.html",
            "relUrl": "/markdown/2022/08/02/segmentation-model-part1.html",
            "date": " • Aug 2, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Hướng dẫn sủ dụng gói thư viện BeautifulSoup trong Python để  Web Scraping",
            "content": "Nội dung của bài viết nhằm hướng dẫn các bạn mới làm quen với đào ảnh từ internet bằng gói thư viện beautiful soup. . Bài viết gồm các ý chính chính sau: . Outline . 1. Giới thiệu về Web Scaping . | 2. Tổng quan cấu trúc web 2.1. HTML, CSS | 2.2. Cấu trúc web | . | 3. Bài toán cụ thể: craping ảnh từ trang web: http://www.globalskinatlas.com/diagindex.cfm 3.1. Cấu trúc của trang web | 3.2. Cách download ảnh thủ công | 3.3. Xây dựng thuật toán từ thác tác down ảnh thủ công | . | 4. Giới thiệu gói thư viện beautiful soup | 5. Hoàn thành code | . 1.Giới thiệu về Web Scaping . Internet có nguồn dữ liệu khổng lồ, dữ liệu mà chúng ta hoàn toàn có thể truy cập bằng cách sử dụng web cùng một công cụ lập trình (Python, C++). Web Scaping là tác vụ download tất cả thông tin liên quan từ một trang web cố định. Ví dụ chúng ta muốn download tất cả các ảnh từ trang web http://www.globalskinatlas.com/diagindex.cfm để làm phong phú kho dữ liệu. . Một số trang web cung cấp cho chúng ta thông qua một API (Application Programming Interface), một số trang web khác có thể co ngừoi dùng lấy dữ liệu thông qua database có sẵn. Ví dụ khi bạn muốn download ảnh từ một trang web, bạn click vào ảnh trên website, từ website sẽ đưa bạn tới một trang web khác, nơi đó có lưu trữ ảnh trực tiếp trên server. . 2. Tổng quan cấu trúc web . Trước khi đi sâu vào làm sao có thể download tất cả dữ liệu từ một trang web, chúng ta sẽ tìm hiểu cấu trúc của một trang web. Việc này giống như đi câu cá, bạn tìm hiểu cấu trúc của hồ nước, để có thêm thông tin giúp việc câu cá dễ dàng hơn. . 2.1 Tổng quan HTML, CSS . Khi chúng ta truy cập một trang web, trình duyệt web (Firefox, Chrome) đưa ra yêu cầu đến máy chủ của trang web. Yêu cầu này được gọi là yêu cầu GET, sau đó chúng ta nhận được thông tin từ máy chủ. Nguồn thông tin từ máy chủ sẽ vẫn được trả lại thông tin gồm những tập file. Nhờ trình duyệt web, các tập này sẽ hiển thị dứoi dạng web. Cấu thành của tập để trình duyệt web có thể đọc một trang web bao gồm: . HTML - nội dung chính của trang. CSS - File này hỗ trợ HTML để hiển thi web đẹp hơn. JS - Các tệp Javascript thêm tính tương tác cho các trang web. Hình ảnh - các định dạng hình ảnh, chẳng hạn như JPG và PNG, cho phép các trang web hiển thị hình ảnh. Sau khi trình duyệt của chúng tôi nhận được tất cả các tệp, nó sẽ hiển thị trang và hiển thị cho chúng tôi. . Ví dụ: Khi chúng ta vào trình duyệt Chrome, chúng ta muốn try tập vào trang http://www.globalskinatlas.com/diagindex.cfm , khi đó máy chủ sẽ trả lại một tập, tập dữ liệu này gồm các file (html, css, javascript,), các file này sẽ được gửi trực tiếp về Chrome, thông qua trình duyệt, tất cả các tệp này sẽ tạp nên một trang web. . Để hiểu rõ cấu trúc một trang web, chúng ta sẽ tìm hiểu sâu file HTML. Ở các trình duyệt. Để hiển thị cấu trúc file HTML, chúng ta bấm phím F12. . 2.2 Tổng quan HTML . Cấu trúc cơ bản của trang HTML có dạng như sau: . - &lt;!Doctype&gt;: Phần khai báo chuẩn của html hay xhtml. - &lt;head&gt;&lt;/head&gt;: Phần khai báo ban đầu, khai báo về meta, title, css, javascript… - &lt;body&gt;&lt;/body&gt;: Phần chứa nội dung của trang web, nơi hiển thị nội dung. . &lt;!DOCTYPE html&gt; &lt;html&gt; &lt;head&gt; &lt;title&gt;Phần tiêu đề của html &lt;/title&gt; &lt;/head&gt; &lt;body&gt; ...Phần thân của html ... &lt;/body&gt; &lt;/html&gt; . Ở phần tiếp theo chúng ta sẽ giới thiệu về thẻ liên kết &lt;a&gt;, một những phần quan trọng nhất để thực hành đào ảnh. . 2.2.1 Thẻ liên kết a . - Thẻ liên kết &lt;a &gt; &lt;/a&gt; dùng để tạo một liên kết từ trang web này sang trang web khác, từ vị trí này sang vị trí khác hay dùng để mở ra một object nào đó (có thể là file words, ảnh, excel, pdf, mp3, movie,...), thẻ này có một thuộc tính bắt buộc: - href: Chứa đường dẫn cụ thể tới mục tiêu liên kết. . Ví dụ: Trong trang web http://www.globalskinatlas.com/diagdetail.cfm?id=91, khi chúng ta sử dụng phím f12, một trong những tag &lt;a&gt; &lt;/a&gt; có dạng như sau . &lt;a href=&quot;imagedetail.cfm?TopLevelid=170&amp;amp;ImageID=462&amp;amp;did=8 &quot;&gt;View&lt;/a&gt; . - Bằng truy cập trang web, ta thấy được liên kết ở tag này là: http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=170&amp;ImageID=462&amp;did=8 - Text để mô tả tag này là *View* . 2.2.2 Thẻ liên kết img . - Thẻ hiển thị một image &lt;*img*/&gt; dùng để nhúng một ảnh thông qua thuộc tính src, thẻ này có 2 thuộc tính bắt buộc: - src: Chứa đường dẫn tham chiếu tới image. - alt: Được sử dụng như một văn bản thay thế khi image không hiển thị (hoặc không có sẵn). . Cấu trúc của thẻ &lt;img&gt; không có sử dụng thẻ đóng (không dùng &lt;img&gt;&lt;/img&gt;), mà sử dụng ký tự kết thúc là một khoảng trắng và ký tự “/”. Tham khảo thêm về thẻ &lt;img/&gt;. . 3.Bài toán cụ thể: craping ảnh từ trang web: http://www.globalskinatlas.com/diagindex.cfm . Ở phần này chúng ta sẽ đi sâu vào phân tích cụ thể và định hướng hướng làm. . Đầu bài: Download tất cả ảnh có ở trang web http://www.globalskinatlas.com/diagindex.cfm và lưu trữ ảnh đó ở thư mục phù hợp. . Nhiệm vụ xuất phát của bài toán xuất phát từ nhu cầu thu thập ảnh bệnh nhân bị bệnh và chúng ta muốn lưu trữ thông tin các bệnh của từng ảnh để tiện sau này sử dụng cho các mô hình về machine learning. . 3.1 Cấu trúc của trang web . Chúng ta cùng xem xét cấu trúc của trang web. Khi thực hiện vào trang web, màn hình sẽ hiện thị như ảnh bên dưới. Chúng ta có thể thấy file được có rất nhiều tag &lt;a&gt;&lt;/a&gt;, mỗi tag tương ứng với một bệnh. . . Bấm phím f12 để thấy được cấu trúc của trang web. Mỗi tag bệnh sẽ tương ứng với . &lt;a href=&quot;diagdetail.cfm?id=653&quot;&gt;&lt;/a&gt; . Nếu click vào một tag, sẽ đưa chúng ta tới một trang web mới, ví dụ ở đây chúng ta click vào Ecthyma, chúng ta được tới trang web http://www.globalskinatlas.com/diagdetail.cfm?id=653 . . Nhận thấy rằng diagdetail.cfm?id=653 sẽ là tương ứng với bệnh Ezthyma. Và id 653 sẽ tương ứng với mã bệnh Ecthyma. Dispay của trang web sẽ có dạng hình như sau . . Khi click vào View ở góc cuối cùng, chúng ta sẽ được tới một trang web mới : http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6 . Trang web mới này được gắn ở href của một tag &lt;a&gt;&lt;/a&gt; của trang web http://www.globalskinatlas.com/diagdetail.cfm?id=653 , cụ thể nội dung của tag &lt;a&gt;&lt;/a&gt;: . &lt;a href=&quot;imagedetail.cfm?TopLevelid=1099&amp;amp;ImageID=2615&amp;amp;did=6&quot;&gt;View&lt;/a&gt; . Có thể thấy “imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6” là tag gắn liền với trang web http://www.globalskinatlas.com/imagedetail.cfm?TopLevelid=1099&amp;ImageID=2615&amp;did=6. . Hiện thị của trang web sẽ có dạng như sau . . Ở trang web này, khi nhấp chuột phải, chúng ta hoàn toàn có thể download ảnh thủ công. Nhưng khi tiến hành download ảnh, chúng ta nhận ra rằng, chỉ có ảnh ở trung tâm có size ảnh lớn, những ảnh nhỏ hơn sẽ có size nhỏ hơn. Vì vậy chúng ta sẽ tiến hành download ảnh ở trung tâm, còn với mỗi ảnh nhỏ ở dưới, chúng ta sẽ nhấn click chuột vào ảnh, ví dụ chúng ta sẽ click vào ảnh nhỏ đầu tiên, từ link ảnh nhỏ sẽ đưa tới trang web có display như sau . . Chúng ta sẽ tiếp tục download ảnh ở trung tâm và lưu ở thư mục bệnh. . 3.2 Thuật toán đào ảnh thủ công . Dựa những phân tích như ở trên, chúng ta xây dựng được thuật toán đào ảnh như sau . Truy cập trang web http://www.globalskinatlas.com/diagindex.cfm 1.1 Truy cập tag bệnh đầu tiên , tạo thư mục bệnh đầu tiên 1.1.1 Truy vào tag View đầu tiên | . | . 1.1.1.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.1.1.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.1.1.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - 1.1.2 Truy vào tag View thứ hia | 1.1.2.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.1.2.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.1.2.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - … - 1.2 Truy cập tag bệnh đầu tiên , tạo thư mục bệnh đầu tiên - 1.2.1 Truy vào tag View đầu tiên | 1.2.1.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.2.1.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.2.1.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - 1.2.2 Truy vào tag View thứ hia | 1.2.2.0 Tải ảnh trung tâm lưu vào folder bệnh | 1.2.2.1 Truy vào ảnh nhỏ đâù tiên và tải ảnh trung tâm lưu vào folder bệnh | 1.2.2.2 Truy vào ảnh nhỏ thứ hai và tải ảnh trung tâm lưu vào folder bệnh - … | . | | . Thuật toán trên đảm bảo giúp chúng ta có thể tải tất cả các ảnh và lưu trữ ảnh ở thư mục phù hợp. Nhiệm vụ của chúng ta bước tiếp theo sẽ tìm hiểu gói thưu viện beautiful soup và thực hiện hoá thuật toán thủ công . 4 Giới thiệu gói thư viện beautiful soup . BeautifulSoup là một gói thư viện của Python nhằm giúp người dùng dễ dàng lấy dữ liệu ra khỏi các file HTML và XML. Bạn đọc có thể tham khảo chi tiết ở trang web https://www.howkteam.vn/d/thu-vien-beautiful-soup-460. . Trong khuôn khô bài viết, chúng ta sẽ tìm hiểu những lệnh cơ bản sau: . Lệnh khởi tạo soup . r = requests.get(web_url) soup = BeautifulSoup(r.content, &quot;html.parser&quot;) . Lệnh lấy tất cả &lt;a&gt; tag . Lệnh trên nhằm tạo món soup dựa trên nguyên liệu trang web web_url, giúp bạn dễ dàng hơn trong việc truy cập dữ liệu của file HTML web_url . links = soup.findall(&quot;a&quot;, href = Trues) . Lấy url ở trong một tag . Lệnh giúp bạn lấy url trong một tag &lt;a&gt;&lt;/a&gt; từ món soup có sẵn. . html_doc = &quot;&quot;&quot; &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &quot;&quot;&quot; from bs4 import BeautifulSoup soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) for a in soup.find_all(&#39;a&#39;, href=True): print (&quot;Found the URL:&quot;, a[&#39;href&#39;]) . Out put trả ra: . Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP Found the URL: /redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh . Lấy text ở trong một tag . Để lấy một tag trong một tag từ soup, chúng ta sử dụng lệnh: . tag.text.strip() . Ví dụ . html_doc = &quot;&quot;&quot; &lt;html&gt;&lt;head&gt;&lt;title&gt;The Dormouse&#39;s story&lt;/title&gt;&lt;/head&gt; &lt;body&gt; &lt;p class=&quot;title&quot;&gt;&lt;b&gt;The Dormouse&#39;s story&lt;/b&gt;&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;Once upon a time there were three little sisters; and their names were &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0L1x0DM4mpSt97%2ftYgbxlC2B7n4pvJNhhvRwo8bxiO4B&quot; class=&quot;sister&quot; id=&quot;link1&quot;&gt;Elsie&lt;/a&gt;, &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0OPun6GIXb9bh0UOloN9WCYbJtHZQd%2fvB08D2UeudkPP&quot; class=&quot;sister&quot; id=&quot;link2&quot;&gt;Lacie&lt;/a&gt; and &lt;a href=&quot;/redirect?Id=f%2fKgPq4IDV0SyEq0zfYr0LirHL60gbBHH3VIishi9CqgtHAKmbGoKNvFheNkumnh&quot; class=&quot;sister&quot; id=&quot;link3&quot;&gt;Tillie&lt;/a&gt;; and they lived at the bottom of a well.&lt;/p&gt; &lt;p class=&quot;story&quot;&gt;...&lt;/p&gt; &quot;&quot;&quot; from bs4 import BeautifulSoup soup = BeautifulSoup(html_doc, &#39;html.parser&#39;) for a in soup.find_all(&#39;a&#39;, href=True): print (&quot;Found the text :&quot;, a.text.strip()) . 5. Hoàn thành code craping dữ liệu cho bài toán . Từ những lệnh cở bản của BeautifulSoup ở mục 4 và thuật toán được xây dựng ở mục 3. Chúng ta xây dựng hàm trên python để craping toàn bộ ảnh bệnh từ trang web http://www.globalskinatlas.com/. . Source code ở link .",
            "url": "https://hphuongdhsp.github.io/ml-blog/2021/11/15/web-scraping.html",
            "relUrl": "/2021/11/15/web-scraping.html",
            "date": " • Nov 15, 2021"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://hphuongdhsp.github.io/ml-blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://hphuongdhsp.github.io/ml-blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi there, I’am Hoang-Phuong. . I am currently working at MedTech Torus-Actions as a Data Scientist. . My passion is in several domains: Machine Learning, Cryptocurrency, Fintech. I like to follow the latest technology in Deep Learning. . Beside works, I like play sports: football, billard, pingpong,… .",
          "url": "https://hphuongdhsp.github.io/ml-blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hphuongdhsp.github.io/ml-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}